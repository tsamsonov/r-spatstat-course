[{"path":"index.html","id":"введение","chapter":"Введение","heading":"Введение","text":"Добро пожаловать на учебный курс “Пространственная статистика на языке R”! Данный курс предназначен для уверенных пользователей R, которые имеют опыт работы с пространственными данными и хотели бы развить свои навыки в области пространственного анализа и моделирования.Материалы данного курса находятся в разработке. Опубликованы черновики глав, которые в дальнейшем будут дополняться и редактироваться, но возможно уже сейчас могут быть кому-то полезны. Если вы заметите какие-либо ошибки, то сообщите о них в баг-трекере курса. Спасибо!Если вы впервые столкнулись с пространственными данными или никогда до этого не программировали на R, то рекомендуется начать с базового курса Визуализация и анализ географических данных на языке R.","code":""},{"path":"index.html","id":"программное-обеспечение","chapter":"Введение","heading":"Программное обеспечение","text":"Для успешного прохождения курса на вашем компьютере должно быть установлено следующее программное обеспечение:Язык RСреда разработки RStudio","code":""},{"path":"index.html","id":"ссылка-на-пособие","chapter":"Введение","heading":"Ссылка на пособие","text":"Если этот курс лекций оказался полезным для вас, и вы хотите процитировать его с списке литературы вашей работы, то ссылку можно оформить по следующей форме:","code":""},{"path":"gstat.html","id":"gstat","chapter":"Глава 1 Геостатистика","heading":"Глава 1 Геостатистика","text":"\\[\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\E}{\\operatorname{E}}\\]","code":""},{"path":"gstat.html","id":"review","chapter":"Глава 1 Геостатистика","heading":"1.1 Краткий обзор","text":"Для просмотра презентации щелкните на ней один раз левой кнопкой мыши и листайте, используя кнопки на клавиатуре:Презентацию можно открыть в отдельном окне или вкладке браузере. Для этого щелкните по ней правой кнопкой мыши и выберите соответствующую команду.","code":""},{"path":"gstat.html","id":"введение-1","chapter":"Глава 1 Геостатистика","heading":"1.2 Введение","text":"Геостатистика — раздел математической статистики, который связан с численным описанием переменных, распределенных в географическом пространстве и, опционально, времени. Наиболее часто инструменты геостатистики используются для решения задачи интерполяции — восстановления сплошного поля распределения случайной величины по ограниченному множеству данных в точках наблюдений. Однако, геостатистика как научная дисциплина существенно шире. Ее первоочередной задачей является статистическое описание пространственных распределений.В основе геостатистики лежит широко разработанный математический аппарат. Понимание основ этого аппарата является необходимым условием осмысленного примения геостатистических методов на практике. В настоящей главе мы постараемся сформировать у читателя данное понимание, и показать, как геостатистика работает на практике.Мир геостатистики базируется на фундаментальных понятиях случайной величины, случайной функции и случайного процесса. Рассмотрим эти понятия.","code":""},{"path":"gstat.html","id":"базовые-понятия-и-элементы-геостатистики","chapter":"Глава 1 Геостатистика","heading":"1.3 Базовые понятия и элементы геостатистики","text":"","code":""},{"path":"gstat.html","id":"базовые-понятия","chapter":"Глава 1 Геостатистика","heading":"1.3.1 Базовые понятия","text":"Отправной точкой геостатистического анализа является конечное множество точек (локаций), в каждой из которых зафиксировано значение некоторой пространственной переменной. Пространственную и атрибутивную составляющую традиционно разделяют на две компоненты, каждая из которых может быть случайной:Пространственные локации (точки)\n\\[\\{p_1, p_2, ..., p_n\\}\\]Данные в этих локациях\n\\[\\{Z(p_1), Z(p_2), ..., Z(p_n)\\}\\]Данные в локациях получаются путем измерений значений пространственно распределенной переменной, или вычисления её значения на основе других данных, которые прямо или косвенно (через другие данные) базируются на прямых или дистанционных наблюдениях. Результаты измерений, как и исходные для расчетов данные, как правило, привязаны ко времени и характеризуют состояние среды на определенный момент. Если описываемое явление является динамическим (изменчивым во времени), результаты наблюдений или расчетов для двух разных моментов времени в общем случае будут различны. Эти различия невозможно полностью описать в аналитическом виде, поскольку природные и социально-экономические процессы формируются неопределенно большим числом факторов. Аналогично этому, невозможно и достоверно предсказать значение пространственной переменной в заданный момент времени. Чтобы работать с такими данными, используются понятия случайной величины и случайного процесса.Случайной величиной \\(Z(w)\\) называется функция, которая в результате случайного события \\(w\\) принимает некоторое вещественнозначное значение.Например, при анализе температуры водоема в отдельно взятой точке в толще воды случайной величиной (функцией) является собственно температура, а событием — та совокупность физико-химических условий, которая сложилась в данной точке в данный момент измерений и обусловила наблюдаемое значение температуры.Отметим, что элемент случайности вносится именно событием, которое в природе может быть сформировано сложной и трудно предсказуемой комбинацией факторов, в то время как случайная величина уже связана с событием некоторой зависимостью, которую можно описать с помощью аналитических или эмпирических формул.Случайность можно наблюдать не только в точке, но и по пространству. Например, уровень шума, формируемый автотранспортом в открытой городской среде, меняется непрерывно, и его можно измерить в каждой точке. При этом пространственное распределение величины этого уровня будет в каждый момент времени зависеть от случайного события — размещения автомобилей и уровня шума, производимого каждым из них. Городская среда оказывается полностью заполнена шумовым эфиром, густота которого неодинакова в пространстве и времени. Перемещаясь из точки в точку или ожидая последующего момента времени, находясь в одной точке, мы будем наблюдать разный уровень шума. Состояние этого шумового эфира как единого целого является случайным процессом.Введем общее понятие случайного процесса:Случайный процесс это семейство случайных величин, индексированных некоторым параметром \\(t\\)Наиболее часто анализируются одномерные случайные процессы, в которых \\(t\\) — это время. Классическим примером такого процесса является количество покупателей, находящихся в магазине.Пространственная статистика изучает случайные процессы, в которых \\(t\\) — это координата точки (обычно на плоскости). Такие процессы характеризуются следующими особенностями:в каждой точке \\(p_i\\) существует некоторая случайная величина \\(Z(p_i)\\) — сечение случайного процессав каждой точке \\(p_i\\) существует некоторая случайная величина \\(Z(p_i)\\) — сечение случайного процессапри изменении точки \\(p_i\\) наблюдаемое значение случайного процесса меняется случайным образом, поскольку определяется оно не только местоположением, но и заранее неизвестным случайным событием.при изменении точки \\(p_i\\) наблюдаемое значение случайного процесса меняется случайным образом, поскольку определяется оно не только местоположением, но и заранее неизвестным случайным событием.Для описания случайных процессов в пространстве необходимо сформировать базовую математическую модель, а также определить ее свойства.","code":""},{"path":"gstat.html","id":"случайный-процесс-в-пространстве-и-его-моменты","chapter":"Глава 1 Геостатистика","heading":"1.3.2 Случайный процесс в пространстве и его моменты","text":"Пусть \\(p \\\\mathbb{R}^k\\) — точка в \\(k\\)-мерном Евклидовом пространстве и \\(Z(p)\\) — случайная величина в точке \\(p\\). Тогда если \\(p\\) меняется в пределах области \\(D \\subset \\mathbb{R}^k\\) (эта область именуется индексным множеством), то формируется случайный процесс:\\[\\{Z(p) | p \\D\\}\\]\nВертикальная черта в соответствии с принятой в теории вероятности нотацией означает условие. То есть, переменная \\(p\\) ограничена областью \\(D\\).Результат наблюдения случайного процесса в точках области \\(D\\) является реализацией случайного процесса:\\[\\{z(p) | p \\D\\}\\]В общем случае \\(D\\) и \\(Z\\) случайны и независимыСлучайный процесс, как и случайную величину, можно описать с помощью статистических моментов, таких как математическое ожидание и дисперсия.Математическое ожидание есть наиболее вероятная реализация случайного процесса:\\[\\E[Z(p)]=m(p)\\]Поясним суть математического ожидания СП на следующем примере:Пусть дан географический регион, в пределах которого рассматривается поле температуры и его временная изменчивость.В каждый момент времени мы имеем непрерывное поле температуры — реализацию случайного процесса.Если рассмотреть поведение это поля во временном разрезе (по аналогии с колебаниями волн в пространстве), то получим некое “среднее” поле — математическое ожидаение случайного процесса.Если в приведенном примере температура наблюдается посредством сети метеостанций, то в каждый момент времени реализацию СП можно приблизительно восстановить путем выполнения интерполяции по их данным. Осреднив же данные по времени, и снова проинтерполировав их, получим выборочную среднюю поверхность — оценку мат. ожидания СП.Дисперсия есть мера разброса реализаций случайного процесса относительно его математического ожидания:\\[\\Var[Z(p)]= \\E[Z^2(p)]-m^2(p)\\]\nАналогично математическому ожиданию, дисперсия двумерного СП представляет собой поле распределения. Величина этого поля каждой точке равна дисперсии сечения СП в этой точке, то есть дисперсии случайной величины. Так же как и в традиционной статистике, вместо дисперсии в расчетах часто используют среднеквадратическое отклонение, поскольку оно выражено в тех же единицах, что и сама случайная величина:\\[\\sigma(p)= \\sqrt{\\Var[Z(p)]}\\]В случае поля температуры можно представить себе объем, ограниченный двумя поверхностями \\(m(p) + \\sigma(p)\\) и \\(m(p) - \\sigma(p)\\). Расстояние между этими поверхностями в каждой точке \\(p\\) представляет собой среднеквадратическое отклонение случайного процесса.Ковариация — это мера линейной зависимости сечений случайного процесса в двух точках \\(p_1\\) и \\(p_2\\):\\[\\Cov(p_1,p_2) = \\Cov[Z(p_1), Z(p_2)] = \\E[Z(p_1)Z(p_2)]-m(p_1)m(p_2)\\]Для вычисления ковариации необходимость знать математическое ожидание СП. Это условие выполняется далеко не всегда, что связано с тем что как правило приходится иметь дело только с одной его реализацией.Следует обратить внимание на то, что моменты пространственных случайных процессов являются функциями, а не константами, в отличие от моментов случайных величин.Давать оценку пространственной структуре явления на основе вычисленных моментов с.п. можно при условии, что он удовлетворяет свойствам стационарности и эргодичности.","code":""},{"path":"gstat.html","id":"стационарность-и-эргодичность","chapter":"Глава 1 Геостатистика","heading":"1.4 Стационарность и эргодичность","text":"","code":""},{"path":"gstat.html","id":"стационарность","chapter":"Глава 1 Геостатистика","heading":"1.4.1 Стационарность","text":"Стационарность в строгом смысле означает что функция распределения множества случайных величин для любой комбинации точек \\({x_1, x_2,...,x_k}\\) и любого \\(k < \\infty\\) остается неизменной при смещении этой комбинации на произвольный вектор \\(h\\):\\[P\\{Z(x_1)<z_1,...,Z(x_k)<z_k\\} = P\\{Z(x_1 + h)<z_1,...,Z(x_k + h)<z_k\\}\\]Стационарность по другому называют однородностью в пространстве, подразумевая что явление ведет себя одинаковым образом в любой точке пространства, как бы повторяет само себя.Стационарность по другому называют однородностью в пространстве, подразумевая что явление ведет себя одинаковым образом в любой точке пространства, как бы повторяет само себя.Если СФ стационарна, все ее моменты будут инвариантны относительно сдвигов (то есть будут постоянны), а это означает что для их оценки можно использовать ограниченную в пространстве область.Если СФ стационарна, все ее моменты будут инвариантны относительно сдвигов (то есть будут постоянны), а это означает что для их оценки можно использовать ограниченную в пространстве область.В реальности подобного рода «идеальное» поведение встречается крайне редко, поэтому используют более слабое предположение о стационарности второго порядка.Случайная функция имеет имеет стационарность второго порядка, если для любых точек \\(x\\) и \\(x+h\\) в \\(R^k\\)\\[\\begin{cases}\n  \\E[Z(x)] = m \\\\\n  \\E[(Z(x)-m)(Z(x+h)-m)] = \\Cov(h)\n\\end{cases}\\]Данные условия означают, что математическое ожидание СФ постоянно, а ковариация зависит только от вектора \\(h\\) между точками и не зависит от их абсолютного положения.Если ковариация также не зависит от направления, а только от расстояния между точками, то \\(h\\) вырождается в скаляр, а такая случайная функция является изотропной стационарной.","code":""},{"path":"gstat.html","id":"эргодичность","chapter":"Глава 1 Геостатистика","heading":"1.4.2 Эргодичность","text":"Стационарная случайная функция \\(Z(x,w)\\) называется эргодической, если ее среднее по области \\(V \\subset R^k\\) сходится к математическому ожиданию \\(m(w)\\) при стремлении \\(V\\) к бесконечности:\\[\\lim_{V \\rightarrow \\infty} \\frac{1}{|V|}\\int_{V} Z(x,w)dx = m(w),\\]\nгде \\(|V|\\) обозначает меру области \\(V\\) (площадь, объем). Предполагается что сама область \\(V\\) растет во всех направлениях, и предел ее роста не зависит от ее формы.Следствием эргодичности является то, что среднее по всем возможным реализациям равно среднему отдельной безграничной в пространстве реализации.Смысл эргодичности можно пояснить на следующем примере. Пусть дан кувшин с песком, в котором необходимо определить долю объема, занятую содержимым. Проведем следующий эксперимент:Зафиксируем некоторую точку \\(x\\) в системе отсчета, привязанной к кувшину, и будем его встряхивать бесконечное число раз, каждый раз фиксируя, оказалась ли точка \\(x\\) внутри песчинки (записываем 1) или же попала в свободное между ними пространство (записываем 0)Из серии подобных экспериментов мы сможем оценить среднее значение индикаторной функции \\((x,w)\\), которое равно вероятности попадания зерна в точку \\(x\\), и которое не зависит от \\(x\\).Эта вероятность и будет равна доли объема кувшина, занятой песком.Аналогичный результат можно получить, если теперь зафиксировать кувшин, а точку \\(x\\) выбирать каждый раз случайным образом. Однако в первом случае берется среднее по реализациям, а во втором среднее по пространству.","code":""},{"path":"gstat.html","id":"геостатистическое-оценивание","chapter":"Глава 1 Геостатистика","heading":"1.5 Геостатистическое оценивание","text":"","code":""},{"path":"gstat.html","id":"простой-кригинг","chapter":"Глава 1 Геостатистика","heading":"1.5.1 Простой кригинг","text":"Для оценки в точке \\(Z_0 = Z(p_0)\\) по \\(N\\) измерениям \\(Z_1, ..., Z_N\\) ищутся коэффициенты \\(\\lambda\\) следующего выражения:\\[Z^* = \\sum_{} \\lambda_i Z_i + \\lambda_0,\\]\nгде константа \\(\\lambda_0 = \\lambda(p_0)\\) и веса \\(\\lambda_i\\) подобираются в точке \\(p_0\\) таким образом, что минимизируется среднеквадратическая ошибка:\\[\\E[Z^* - Z_0]^2,\\]\nто есть, математическое ожидание квадрата отклонения оценки от теоретического значения.Согласно традиции, принятой в литературе по геостатистике, оценка в точке \\(p_0\\) обозначается звездочкой (\\(Z^*\\)), а истинное (неизвестное) значение нулевым индексом (\\(Z_0\\)).В целях уменьшения количества скобок мы используем нотацию \\(\\operatorname{E}[Z^* - Z_0]^2 = \\operatorname{E}\\big[(Z^* - Z_0)^2\\big]\\)Теоретическое значение \\(Z_0\\), входящее в формулу ошибки, не известно. Однако, как будет показано далее, его знание и не требуется, поскольку ищется не сам квадрат отклонения, а его математическое ожидание.Используя соотношение \\(\\Var[X] = \\E[X^2] - (\\E[X])^2\\), выразим среднюю квадратическую ошибку как:\\[\\E[Z^* - Z_0]^2 = \\Var[Z^* - Z_0] + \\big(\\E[Z^* - Z_0]\\big)^2\\]Поскольку дисперсия нечувствительна к сдвигам, изменение константы \\(\\lambda_0\\) влияет только на компоненту \\(\\E[Z^* - Z_0]\\). Приравняем ее нулю:\\[\\E[Z^* - Z_0] = \\E\\Big[\\sum_{} \\lambda_i Z_i + \\lambda_0 - Z_0\\Big] = 0\\]Используя свойства математического ожидания, выразим из этого выражения \\(\\lambda_0\\):\\[\\lambda_0 = - \\E\\Big[\\sum_{} \\lambda_i Z_i - Z_0\\Big] = \\E[Z_0] - \\sum_{} \\lambda_i \\E[Z_i] = m_0 - \\sum_i \\lambda_i m_i,\\]где \\(m_i\\) — теоретически известные значения мат. ожидания случайной функции в точках исходных данных \\(p_i\\), \\(m_0\\) — теоретически известное мат. ожидание случайной функции в оцениваемой точке \\(p_0\\).Имея:\\[Z^* = \\sum_{} \\lambda_i Z_i + \\lambda_0,\\\\\n\\lambda_0 = m_0 - \\sum_i \\lambda_i m_i,\\]Получаем:\\[Z^* = \\sum_{} \\lambda_i Z_i + m_0 - \\sum_i \\lambda_i m_i = m_0 + \\sum_{} \\lambda_i (Z_i - m_i)\\]\nПоскольку математические ожидания \\(m_0\\) и \\(m_i\\) предполагаются известными, величину \\(Z(p)\\) можно заменить величиной \\(Y(p) = Z(p) - m(p)\\) в обеих частях уравнения (это эквивалентно вычитанию известного тренда из исходных измерений):\\[\nZ^* = m_0 + \\sum_{} \\lambda_i (Z_i - m_i),\\\\\n\\underbrace{Z^* - m_0}_{Y^*} = \\sum_{} \\lambda_i \\underbrace{(Z_i - m_i)}_{Y_i},\\\\\nY^* =  \\sum_{} \\lambda_i Y_i.\n\\]\nЭто означает, что оценку \\(Z^*\\) можно заменить оценкой \\(Y^*\\) и прибавлением к ней среднего значения \\(m_0\\):\\[Z^* = m_0 + \\sum_{} \\lambda_i Y_i,\\]Обратим внимание на то, что \\(\\E[Y^* - Y_0] = \\underbrace{\\E[Y^*]}_0 - \\underbrace{\\E[Y_0]}_0 = 0\\) по определению \\(Y(p)\\). В этом случае также \\(\\lambda_0 = \\underbrace{\\E[Y_0]}_0 - \\sum_{} \\lambda_i \\underbrace{\\E[Y_i]}_0 = 0\\) .Чтобы не загромождать дальнейшее изложение формулами, примем, что \\(Z^* := Y^*\\), предполагая, что исходная величина уже центрирована относительно мат. ожидания, и к результату вычислений его надо прибавить.Поскольку \\(\\E[Z^* - Z_0] = 0\\), среднеквадратическую ошибка будет равна дисперсии:\\[\\E[Z^* - Z_0]^2 = \\Var[Z^* - Z_0]\\]\nВыразим дисперсию разностей в терминах статистических моментов исходной функции. Для этого воспользуемся следующими свойствами дисперсии и ковариации:\\(\\Var[X + Y] = \\Var[X] + \\Var[Y] + 2\\Cov[X, Y]\\);\\(\\Var[-X] = \\Var[X]\\);\\(\\Cov[X, -Y] = -\\Cov[X, Y]\\).Используя эти свойства, получаем:\\[\\E[Z^* - Z_0]^2 = \\Var[Z^* - Z_0] = \\Var[Z^*] + \\Var[Z_0] - 2\\Cov[Z^*, Z_0].\\]Чтобы минимизировать данное выражение, необходимо раскрыть содержание трёх его компонент. Для этого нам понадобится следующая теорема, позволяющая выразить ковариацию двух линейных комбинаций случайных величин через ковариацию самих исходных случайных величин:Теорема 1.1  Пусть \\(X_1,\\ldots, X_n\\) случайные величины, а \\(Y_1 = \\sum\\limits_{=1}^n a_i X_i,\\; Y_2 = \\sum\\limits_{j=1}^m b_j X_j\\) — их две произвольные линейные комбинации. Тогда:\\[\\Cov[Y_1,Y_2] = \\sum\\limits_{=1}^n\\sum\\limits_{j=1}^m a_i b_j \\Cov[X_i,X_j].\\]Используя результат этой теоремы, а также тот факт, что \\(\\Var[X] = \\Cov[X, X]\\), распишем каждую компоненту вышеприведенного выражения:\\(\\Var[Z^*] = \\Cov[Z^*, Z^*] = \\Cov\\Big[\\sum_{} \\lambda_i Z_i, \\sum_{j} \\lambda_j Z_j\\Big] =\\\\\\sum_{}\\sum_{j} \\lambda_i \\lambda_j \\Cov[Z_i, Z_j] = \\sum_{}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij},\\)\\(\\Var[Z^*] = \\Cov[Z^*, Z^*] = \\Cov\\Big[\\sum_{} \\lambda_i Z_i, \\sum_{j} \\lambda_j Z_j\\Big] =\\\\\\sum_{}\\sum_{j} \\lambda_i \\lambda_j \\Cov[Z_i, Z_j] = \\sum_{}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij},\\)\\(\\Var[Z_0] = \\Cov[Z_0, Z_0] = \\sigma_{00},\\)\\(\\Var[Z_0] = \\Cov[Z_0, Z_0] = \\sigma_{00},\\)\\(\\Cov[Z^*, Z_0] = \\Cov\\Big[\\sum_{} \\lambda_i Z_i, Z_0\\Big] = \\sum_{} \\lambda_i \\Cov[Z_i, Z_0] = \\sum_{} \\lambda_i \\sigma_{i0},\\)\\(\\Cov[Z^*, Z_0] = \\Cov\\Big[\\sum_{} \\lambda_i Z_i, Z_0\\Big] = \\sum_{} \\lambda_i \\Cov[Z_i, Z_0] = \\sum_{} \\lambda_i \\sigma_{i0},\\)где \\(\\sigma_{ij}\\) — ковариация случайных величин в точках \\(p_i\\) и \\(p_j\\), \\(\\sigma_{i0}\\) — ковариация случайных величин в точках \\(p_i\\) и оцениваемой точке \\(p_0\\), \\(\\sigma_{00}\\) — дисперсия в точке \\(p_0\\).Используя полученные соотношения, выражение для ошибки\\[\\E[Z^* - Z_0]^2 = \\Var[Z^*] + \\Var[Z_0] - 2\\Cov[Z^*, Z_0]\\]можно представить следующим образом:\\[\\E[Z^* - Z_0]^2 = \\sum_{}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\\]Для нахождения минимума этой квадратичной функции необходимо приравнять нулю ее производные по основной переменной \\(\\lambda\\). Выберем в качестве «жертвы» коэффициенты с индексом \\(\\):\\[\\frac{\\partial}{\\partial \\lambda_i} \\E[Z^* - Z_0]^2 = 2 \\sum_{j} \\lambda_j \\sigma_{ij} - 2 \\sigma_{i0} = 0\\]Таким образом, система уравнений простого кригинга для точки \\(Z_0\\) имеет вид:\\[\\color{red}{\\boxed{\\color{blue}{\\sum_{j} \\lambda_j \\sigma_{ij} = \\sigma_{i0}\\color{gray}{,~= 1,...,N}}}}\\]Полученные уравнения простого кригинга носят чисто теоретический характер, поскольку предполагают знание ковариационной матрицы \\(\\Sigma = \\{\\sigma_{ij}\\}\\) случайного процесса. Тем не менее, на их основе удобно выводить уравнения обычного (ординарного) кригинга, в котором подобное знание уже не требуется.","code":""},{"path":"gstat.html","id":"дисперсия-простого-кригинга","chapter":"Глава 1 Геостатистика","heading":"1.5.2 Дисперсия простого кригинга","text":"Существует возможность оценить в каждой точке не только величину показателя, но также дисперсию оценки (в случае постоянного мат. ожидания — среднеквадратическую ошибку).Для этого необходимо коэффициенты \\(\\lambda_i\\), полученные из системы уравнения простого кригинга\\[\\sum_{j} \\lambda_j \\sigma_{ij} = \\sigma_{i0}\\]подставить в выражение среднеквадратической ошибки\\[\\Var[Z^* - Z_0] = \\sum_{}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\\]Умножим обе части каждого уравнения простого кригинга на \\(\\lambda_i\\) и просуммируем все уравнения по \\(\\):\\[\\sum_{j} \\lambda_j \\sigma_{ij} = \\sigma_{i0}~\\Bigg|\\times \\lambda_i\\\\\n\\color{red}{\\sum_{}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij}} = \\color{blue}{\\sum_{}\\lambda_i\\sigma_{i0}}\\]Заметим, что левая часть уравнения присутствует в выражении среднеквадратической ошибки:\\[\\Var[Z^* - Z_0] = \\color{red}{\\sum_{}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij}} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\\]Выполним соответствующую замену \\(\\sum_{}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij}\\) на \\(\\sum_{}\\lambda_i\\sigma_{i0}\\):\\[\\Var[Z^* - Z_0] = \\color{blue}{\\sum_{}\\lambda_i\\sigma_{i0}} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\\]Отсюда получаем выражение для дисперсии (ошибки) простого кригинга:\\[\\color{red}{\\boxed{\\color{blue}{\\sigma_{SK} = \\Var[Z^* - Z_0] = \\sigma_{00} - \\sum_{}\\lambda_i\\sigma_{i0}}}}\\]","code":""},{"path":"gstat.html","id":"стационарность-приращений","chapter":"Глава 1 Геостатистика","heading":"1.5.3 Стационарность приращений","text":"Стационарность второго порядка требует знания математического ожидания для вычисления ковариации.В ряде случаев оценить математическое ожидание оказывается невозможно (оно может не существовать) или же оно действительно оказывается непостоянным.Тогда пользуются еще более мягкой формой стационарности стационарностью приращений, при которой стационарной предполагается не сама с.ф. \\(Z(x)\\), а производная от нее функция:\\[Y_h(x) = Z(x+h)-Z(x)\\]Функция \\(Z(x)\\), обладающая таким свойством, называется подчиняющейся внутренней гипотезе.У функции \\(Y_h(x) = Z(x+h)-Z(x)\\) должны существовать математическое ожидание и дисперсия приращений:\\[\\begin{cases}\n\\E[Z(x+h)-Z(x)] = \\langle ,h \\rangle \\\\\n\\Var[Z(x+h)-Z(x)] = 2\\gamma(h)\n\\end{cases}\\]\\(\\langle ,h \\rangle\\) обозначает линейный тренд \\(\\) при заданном векторе \\(h\\) (математическое ожидание разности значений), который варажется через скалярное произведение: \\(\\langle ,h \\rangle = \\sum_i a_i h_i\\)\\(\\langle ,h \\rangle\\) обозначает линейный тренд \\(\\) при заданном векторе \\(h\\) (математическое ожидание разности значений), который варажется через скалярное произведение: \\(\\langle ,h \\rangle = \\sum_i a_i h_i\\)\\(\\gamma(h)\\) — дисперсия приращений, называемая вариограммой\\(\\gamma(h)\\) — дисперсия приращений, называемая вариограммойЕсли процесс подчиняется гипотезе стационарности второго рода \\(\\E[Z(x)] = m\\), то \\(\\E[Z(x+h)-Z(x)] = \\E[Y_h(x)] = 0\\) и вариограмму можно выразить следующим образом:\\[2\\gamma(h) = \\Var[Z(x+h)-Z(x)] = \\Var[Y_h(x)] \\\\=\\E\\big[Y_h(x)\\big]^2 - \\Big(\\E\\big[Y_h(x)\\big]\\Big)^2 \\\\=\\E\\big[Y_h(x)\\big]^2 = \\E\\big[Z(x+h)-Z(x)\\big]^2\\]Таким образом, наиболее распространенная в геостатистике гипотеза подчиняется следующим условиям:\\[\\begin{cases}\n\\E\\big[Z(x)\\big] = m\\\\\n\\E\\big[Z(x+h)-Z(x)\\big] = 0 \\\\\n\\E\\big[Z(x+h)-Z(x)\\big]^2 = 2\\gamma(h)\n\\end{cases}\\]Эти условия позволяют избавиться от необходимости знания среднего значения и дисперсии случайной функции и использовать для вычислений вариограмму.Эти условия позволяют избавиться от необходимости знания среднего значения и дисперсии случайной функции и использовать для вычислений вариограмму.Чтобы модифицировать соответствующим образом уравнения простого кригинга, необходимо знать связь между ковариацией и вариограммой.Чтобы модифицировать соответствующим образом уравнения простого кригинга, необходимо знать связь между ковариацией и вариограммой.","code":""},{"path":"gstat.html","id":"переход-от-ковариации-к-вариограмме","chapter":"Глава 1 Геостатистика","heading":"1.5.4 Переход от ковариации к вариограмме","text":"","code":""},{"path":"gstat.html","id":"предварительные-условия","chapter":"Глава 1 Геостатистика","heading":"1.5.4.1 Предварительные условия","text":"Для того чтобы функция могла считаться ковариацией, необходимо, чтобы дисперсия, вычисленная на ее основе, была положительной:\\[\\Var \\Bigg[\\sum_{=1}^N \\lambda_i Z(x_i)\\Bigg] = \\Cov\\Bigg[\\sum_{=1}^N \\lambda_i Z(x_i), \\sum_{j=1}^N \\lambda_j Z(x_j)\\Bigg] = \\\\=\\sum_{=1}^N \\sum_{j=1}^N \\lambda_i \\lambda_j \\Cov\\big[Z(x_i), Z(x_j)\\big] \\\\= \\sum_{=1}^N \\sum_{j=1}^N \\lambda_i \\lambda_j C(x_j - x_i)\\]Функция \\(C(h)\\), для которой при любых значениях \\(N\\), \\(x_i\\) и \\(\\lambda_i\\) выражение \\(\\sum_{=1}^N \\sum_{j=1}^N \\lambda_i \\lambda_j C(x_j - x_i)\\) принимает неотрицательные значения, называется положительно определенной.Если функция отвечает внутренней гипотезе, нет гарантий, что ее ковариация существует и ограничена. В этом случае можно оценить дисперсию суммы случайных функций через дисперсию приращений, наложив дополнительное условие \\(\\sum_{=1}^N \\lambda_i = 0\\). Учитывая что \\(\\sum_{=1}^N \\lambda_i Z(x_0) = Z(x_0) \\sum_{=1}^N \\lambda_i = 0\\), имеем:\\[\\sum_{=1}^N \\lambda_i Z(x_i) = \\sum_{=1}^N \\lambda_i Z(x_i) - \\sum_{=1}^N \\lambda_i Z(x_0) = \\sum_{=1}^N \\lambda_i \\big[Z(x_i) - Z(x_0)\\big]\\]Линейные комбинации, отвечающие условию \\(\\sum_{=1}^N \\lambda_i = 0\\), называются допустимыми линейными комбинациями.","code":""},{"path":"gstat.html","id":"вывод-выражения-для-ковариации-через-вариограмму","chapter":"Глава 1 Геостатистика","heading":"1.5.4.2 Вывод выражения для ковариации через вариограмму","text":"Рассмотрим ковариацию двух линейных комбинаций с.ф.:\\[\\Cov \\Bigg[\\sum_{=1}^N \\lambda_i Z(x_i), \\sum_{j=1}^M \\mu_j Z(x_j) \\Bigg] = \\sum_{=1}^N \\sum_{j=1}^M \\lambda_i \\mu_j C(x_j - x_i)\\]Используя правило \\(\\Cov[X + \\alpha, Y + \\beta] = \\Cov[X, Y]\\), введем условное начало координат:\\[\\sum_{=1}^N \\sum_{j=1}^M \\lambda_i \\mu_j C(x_j - x_i) =\\\\\n=\\sum_{=1}^N \\sum_{j=1}^M \\lambda_i \\mu_j \\Cov \\big[Z(x_i) - Z(x_0), Z(x_j) - Z(x_0)\\big]\\]Распишем ковариацию через математические ожидания, учитывая, что, согласно гипотезе, \\(\\E\\big[Z(x+h)-Z(x)\\big] = 0\\):\\[\\Cov \\big[Z(x_i) - Z(x_0), Z(x_j) - Z(x_0)\\big] =\\\\\n= \\E\\big[Z_i - Z_0\\big]\\big[Z_j - Z_0\\big] - \\E\\big[Z_i - Z_0\\big] E\\big[Z_j - Z_0\\big] = \\\\\n= \\E\\big[Z_i - Z_0\\big]\\big[Z_j - Z_0\\big]\\]Обратим внимание, что произведение приращений можно выразить через квадраты приращений:\\[\\color{blue}{(Z_j - Z_i)^2} = \\big[(Z_j - Z_0) - (Z_i - Z_0)\\big]^2 = \\\\ = \\color{blue}{(Z_i - Z_0)^2} - 2\\color{red}{(Z_i - Z_0)(Z_j - Z_0)} + \\color{blue}{(Z_j - Z_0)^2}\\]Имеем:\\[ (Z_i - Z_0)(Z_j - Z_0) = \\frac{1}{2} \\Big[(Z_i - Z_0)^2 + (Z_j - Z_0)^2 - (Z_j - Z_i)^2\\Big]\\]\nУчитывая, что \\(\\E\\big[Z(x+h)-Z(x)\\big]^2 = 2\\gamma(h)\\), подставим это выражение в формулу вычисления ковариации:\\[\\Cov \\big[Z(x_i) - Z(x_0), Z(x_j) - Z(x_0)\\big] = \\E\\big[Z_i - Z_0\\big]\\big[Z_j - Z_0\\big] = \\\\\n= \\frac{1}{2} \\E \\Big[(Z_i - Z_0)^2 + (Z_j - Z_0)^2 - (Z_j - Z_i)^2\\Big] = \\\\\n= \\gamma(x_i - x_0) + \\gamma(x_j - x_0) - \\gamma(x_j - x_i)\\]Подставим полученное выражение в двойную сумму:\\[\\sum_{=1}^N \\sum_{j=1}^M \\lambda_i \\mu_j \\Cov \\big[Z(x_i) - Z(x_0), Z(x_j) - Z(x_0)\\big] = \\\\\n= \\sum_{=1}^N \\sum_{j=1}^M \\lambda_i \\mu_j \\big[\\gamma(x_i - x_0) + \\gamma(x_j - x_0) - \\gamma(x_j - x_i)\\big] = \\\\\n= \\underbrace{\\color{red}{\\sum_{=1}^N \\sum_{j=1}^M \\lambda_i \\mu_j \\gamma(x_i - x_0)}}_0 + \\underbrace{\\color{blue}{\\sum_{=1}^N \\sum_{j=1}^M \\lambda_i \\mu_j \\gamma(x_j - x_0)}}_0 - \\sum_{=1}^N \\sum_{j=1}^M \\lambda_i \\mu_j \\gamma(x_j - x_i) = \\\\\n= - \\sum_{=1}^N \\sum_{j=1}^M \\lambda_i \\mu_j \\gamma(x_j - x_i),\\]Компоненты суммы, содержащие \\(x_0\\), будут равны нулю, поскольку они содержат коэффициенты с индексами, не участвующими в вычислении \\(\\gamma\\) — суммирование по этим индексам можно вынести наружу:\\(\\color{red}{\\sum_{=1}^N \\sum_{j=1}^M \\lambda_i \\mu_j \\gamma(x_i - x_0)} = \\underbrace{\\sum_{j=1}^M \\mu_j}_0 \\sum_{=1}^N \\lambda_i \\gamma(x_i - x_0) = 0;\\)\\(\\color{blue}{\\sum_{=1}^N \\sum_{j=1}^M \\lambda_i \\mu_j \\gamma(x_j - x_0)} = \\underbrace{\\sum_{=1}^N \\lambda_i}_0 \\sum_{j=1}^M \\mu_i \\gamma(x_j - x_0) = 0,\\)где соотношения \\(\\sum_{=1}^N \\lambda_i = 0\\) и \\(\\sum_{j=1}^M \\mu_j = 0\\) соблюдаются в силу того, что мы работаем с допустимыми линейными комбинациями.Согласно полученным выкладкам у нас появляется возможность сравнить выражение для ковариации двух линейных комбинаций случайных величин для стационарного случая и внутренней гипотезы:\\[\\Cov \\Bigg[\\sum_{=1}^N \\lambda_i Z(x_i), \\sum_{j=1}^M \\mu_j Z(x_j) \\Bigg] = \\underbrace{\\sum_{=1}^N \\sum_{j=1}^M \\lambda_i \\mu_j \\sigma(x_j - x_i)}_{\\texttt{стационарный случай}} = \\underbrace{- \\sum_{=1}^N \\sum_{j=1}^M \\lambda_i \\mu_j \\gamma(x_j - x_i)}_{\\texttt{внутренняя гипотеза}}\\]Видно, данные выражения отличаются лишь знаком (в случае внутренней гипотезы появляется минус) и вычисляемой функцией (ковариация заменяется на вариограмму при переходе к внутренней гипотезе).Таким образом, получаем важнейший вывод, необходимый для решения системы уравнений обычного кригинга, рассматриваемых в следующем параграфе:При соблюдении внутренней гипотезы в уравнениях кригинга можно принять \\(\\sigma_{ij} = -\\gamma_{ij}\\)Следствием этого также является то, что дисперсия допустимой линейной комбинации может быть выражена через вариограмму:\\[\\Var \\Bigg[\\sum_{=1}^N \\lambda_i Z(x_i)\\Bigg] = - \\sum_{=1}^N \\sum_{j=1}^N \\lambda_i \\lambda_j \\gamma(x_j - x_i)\\]Функция \\(G(h)\\), для которой при условии и \\(\\sum_{=1}^N \\lambda_i = 0\\) выражение \\(\\sum_{=1}^N \\sum_{j=1}^N \\lambda_i \\lambda_j G(x_j - x_i)\\) принимает неотрицательные значения, называется условно положительно определенной.\\(-\\gamma(h)\\) — условно положительно определенная ф.","code":""},{"path":"gstat.html","id":"обычный-кригинг","chapter":"Глава 1 Геостатистика","heading":"1.5.5 Обычный кригинг","text":"Пусть дано неизвестное среднее \\(m(x) = a_0\\). Необходимо произвести линейную оценку \\(Z^* = \\sum_{} \\lambda_i Z_i + \\lambda_0\\). Выразим среднюю квадратическую ошибку:\\[\\E\\big[(Z^* - Z_0)^2\\big] = \\Var[Z^* - Z_0] + \\big(\\E[Z^* - Z_0]\\big)^2 =\\\\\n= \\Var[Z^* - Z_0] + \\Bigg[\\E\\bigg[\\sum_{} \\lambda_i Z_i + \\lambda_0\\bigg] - \\E\\big[Z_0\\big] \\Bigg]^2 =\\\\= \\Var[Z^* - Z_0] + \\Bigg[\\underbrace{\\E[\\lambda_0]}_{\\lambda_0} + \\sum_{} \\lambda_i \\underbrace{\\E\\big[Z_i\\big]}_{a_0} - \\underbrace{\\E\\big[Z_0\\big]}_{a_0} \\Bigg]^2 =\\\\= \\Var[Z^* - Z_0] + \\Bigg[\\lambda_0 + \\bigg(\\sum_i \\lambda_i - 1 \\bigg) a_0 \\Bigg]^2\\]Только компонента сдвига \\(\\E[Z^* - Z_0]\\) содержит \\(\\lambda_0\\), однако, в отличие от случая простого кригинга, мы не можем минимизировать ее, не зная \\(a_0\\).Единственный способ избавиться от \\(a_0\\) заключается в том, чтобы наложить дополнительное условие \\(\\sum \\lambda_i - 1 = 0\\)Минимизируем ранее введенную функцию ошибки:\\[\\Var[Z^* - Z_0] = \\sum_{}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\\]Для этого, с учетом дополнительного условия \\(\\sum \\lambda_i -1 = 0\\) применим метод множителей Лагранжа и построим вспомогательную функцию:\\[Q = \\Var[Z^* - Z_0] + 2\\mu \\bigg(\\sum_i \\lambda_i - 1 \\bigg),\\]где \\(\\mu\\) – неизвестный множитель Лагранжа.Для минимизации функции приравняем нулю ее частные производные:\\[\\begin{cases}\\frac{\\partial Q}{\\partial \\lambda_i} = 2 \\sum_j \\lambda_j \\sigma_{ij} - 2 \\sigma_{i0} + 2\\mu = 0,~= 1,...,N,\\\\\n\\frac{\\partial Q}{\\partial \\mu} = 2\\bigg(\\sum_i \\lambda_i - 1 \\bigg) = 0\n\\end{cases}\\]Имеем \\(N + 1\\) уравнений с \\(N + 1\\) неизвестными:\\[\\begin{cases}\\sum_j \\lambda_j \\sigma_{ij} + \\mu = \\sigma_{i0},~= 1,...,N,\\\\\n\\sum_i \\lambda_i = 1\n\\end{cases}\\]Заменяя ковариацию на вариограмму, согласно выводу, полученному в предыдущем параграфе, получаем систему уравнений обычного кригинга:\\[\\color{red}{\\boxed{\\color{blue}{\\begin{cases}\\sum_j \\lambda_j \\gamma_{ij} - \\mu = \\gamma_{i0},\\color{gray}{~= 1,...,N,}\\\\\n\\sum_i \\lambda_i = 1\n\\end{cases}}}}\\]Обычный (или ординарный) кригинг — наиболее часто используемый геостатистический метод интерполяции.","code":""},{"path":"gstat.html","id":"дисперсия-обычного-кригинга","chapter":"Глава 1 Геостатистика","heading":"1.5.6 Дисперсия обычного кригинга","text":"Вывод формулы для оценки дисперсии обычного кригинга выполняется аналогично случаю простого кригинга. Умножим \\(N\\) первых уравнений на \\(\\lambda_i\\), просуммируем их по \\(\\):\\[\\sum_j \\lambda_j \\gamma_{ij} - \\mu = \\gamma_{i0}~\\Bigg|\\times \\lambda_i\\]Учтя дополнительное условие \\(\\sum_i \\lambda_i = 1\\), получаем выражение для оценки дисперсии (ошибки) обычного кригинга:\\[\\color{red}{\\boxed{\\color{blue}{\\sigma_{OK} = Var[Z^* - Z_0] = \\sum_{}\\lambda_i\\gamma_{i0} - \\mu}}}\\]","code":""},{"path":"gstat.html","id":"универсальный-кригинг","chapter":"Глава 1 Геостатистика","heading":"1.5.7 Универсальный кригинг","text":"В методе универсального кригинга осуществляется декомпозиция переменной \\(Z(x)\\) в виде следующей суммы:\\[Z(x) = m(x) + Y(x)\\]\\(m(x)\\) — дрифт (drift), гладкая детерминированная функция, описывающая систематическую составляющую пространственной изменчивости явления;\\(m(x)\\) — дрифт (drift), гладкая детерминированная функция, описывающая систематическую составляющую пространственной изменчивости явления;\\(Y(x)\\) - остаток (residual), случайная функция с нулевым математическим ожиданием, описывающая случайную составляющую пространственной изменчивости явления;\\(Y(x)\\) - остаток (residual), случайная функция с нулевым математическим ожиданием, описывающая случайную составляющую пространственной изменчивости явления;Декомпозиция любого явления на дрифт и остаток зависит от масштаба рассмотрения явления.Метод универсального кригинга используется, когда математическое ожидание случайного процесса непостоянно по пространству. Это позволяет интерполировать данные, в которых присутствует тренд.В предположении, что м.о. имеет функциональную зависимость от других процессов в точке \\(x\\), вводится следующая модель:\\[m(x) = \\sum_{k=0}^{K} a_k f^k(x),\\]где \\(f^k(x)\\) — известные базисные функции, а \\(a_k\\) — фиксированные для точки \\(x\\), но неизвестные коэффициенты.Обычно первая базисная функция при \\(k = 0\\) представляет собой константу, равную 1. Это позволяет включить в модель случай постоянного м.о.Обычно первая базисная функция при \\(k = 0\\) представляет собой константу, равную 1. Это позволяет включить в модель случай постоянного м.о.Если среднее зависит только от местоположения, то оставшиеся функции \\(f^k(x), k > 0\\), как правило, представляют собой одночлены от координат (например, для двумерного случая \\(f^2(p) = x^2 + y^2\\))Если среднее зависит только от местоположения, то оставшиеся функции \\(f^k(x), k > 0\\), как правило, представляют собой одночлены от координат (например, для двумерного случая \\(f^2(p) = x^2 + y^2\\))Коэффициенты \\(a_k\\) могут меняться в зависимости от \\(x\\), но обязательно медленно, чтобы их можно было считать постоянными в окрестности \\(x\\).Коэффициенты \\(a_k\\) могут меняться в зависимости от \\(x\\), но обязательно медленно, чтобы их можно было считать постоянными в окрестности \\(x\\).В качестве дрифта \\(m(x)\\) можно использовать не только функцию от местоположения, но также значения внешней переменной — ковариаты.Например, количество осадков можно связать с высотой точки \\(H(x)\\) следующей моделью:\\[Z(x) = a_0 + a_1 H(x) + Y(x)\\]С статистической точки зрения это линейная регрессия, в которой остатки коррелированы (автокоррелированы).В литературе данный метод называют также регрессионным кригингом (regression kriging), а оценка дрифта — пространственной регрессией (spatial regression).Для вывода уравнений рассмотрим среднеквадратическую ошибку:\n\\[\\E[Z^* - Z_0]^2 = \\Var[Z^* - Z_0] + \\big(\\E[Z^* - Z_0]\\big)^2\\]Используя введенную модель дрифта \\(m(x) = \\sum_{k=0}^{K} ^k f^k(x)\\) распишем выражение для мат.ожидания приращений:\\[\\E[Z^* - Z_0] = \\E[Z^*\\big] - \\E[Z_0] = \\\\\n\\sum_i \\lambda_i \\sum_k a_k f_i^k - \\sum_k a_k f_0^k = \\sum_k a_k \\Bigg(\\sum_i \\lambda_i f_i^k - f_0^k\\Bigg)\\]Чтобы минимизировать \\(\\E[Z^* - Z_0]\\) независимо от коэффициентов \\(a_k\\), достаточно в вышеприведенной формуле приравнять нулю выражение в скобках. Отсюда имеем:\\[\\sum_i \\lambda_i f_i^k = f_0^k,~k = 0, 1, ..., K.\\]Эти условия называются условиями универсальности. Отсюда идет название метода — универсальный кригингУсловия универсальности гаранируют, что оценка \\(Z^*\\) является несмещенной для любых значений \\(a_k\\).Минимизируем ранее введенную функцию ошибки:\\[\\Var[Z^* - Z_0] = \\sum_{}\\sum_{j} \\lambda_i \\lambda_j \\sigma_{ij} - 2 \\sum_{} \\lambda_i \\sigma_{i0} + \\sigma_{00}\\]Для этого, с учетом дополнительного условия \\(\\sum_i \\lambda_i f_i^k = f_0^k\\) применим метод множителей Лагранжа и построим вспомогательную функцию:\\[Q = \\Var[Z^* - Z_0] + 2 \\sum_{k=0}^K \\mu_k \\Bigg[ \\sum_i \\lambda_i f_i^k - f_0^k\\Bigg],\\]где \\(\\mu_k,~k = 0, 1, ..., K\\) представляют \\(K + 1\\) дополнительных неизвестных, множители Лагранжа.Для минимизации функции приравняем нулю ее частные производные:\\[\\begin{cases}\\frac{\\partial Q}{\\partial \\lambda_i} = 2 \\sum_j \\lambda_j \\sigma_{ij} -2 \\sigma_{i0} + 2 \\sum_k \\mu_k f_i^k = 0,\\color{gray}{~= 1,...,N,}\\\\\n\\frac{\\partial Q}{\\partial \\mu} = 2\\bigg[\\sum_i \\lambda_i f_i^k - f_0^k \\bigg] = 0\\color{gray}{,~k = 0, 1,..., K.}\n\\end{cases}\\]Имеем систему из \\(N + K + 1\\) уравнений с \\(N + K + 1\\) неизвестными:\\[\\begin{cases}\\sum_j \\lambda_j \\sigma_{ij} + \\sum_k \\mu_k f_i^k = \\sigma_{i0},~= 1,...,N,\\\\\n\\sum_i \\lambda_i f_i^k = f_0^k,~k = 0, 1,..., K.\n\\end{cases}\\]Заменяя ковариацию на вариограмму, получаем систему уравнений универсального кригинга:\\[\\color{red}{\\boxed{\\color{blue}{\\begin{cases}\\sum_j \\lambda_j \\gamma_{ij} - \\sum_k \\mu_k f_i^k = \\gamma_{i0},\\color{gray}{~= 1,...,N,}\\\\\n\\sum_i \\lambda_i f_i^k = f_0^k\\color{gray}{,~k = 0, 1,..., K.}\n\\end{cases}}}}\\]","code":""},{"path":"gstat.html","id":"дисперсия-универсального-кригинга","chapter":"Глава 1 Геостатистика","heading":"1.5.8 Дисперсия универсального кригинга","text":"Вывод формулы для оценки дисперсии универсального кригинга выполняется аналогично случаю обычного кригинга. Умножим \\(N\\) первых уравнений на \\(\\lambda_i\\), просуммируем их по \\(\\):\\[\\sum_j \\lambda_j \\gamma_{ij} - \\sum_k \\mu_k f_i^k = \\gamma_{i0} ~ \\Bigg|\\times \\lambda_i\\]Учтя дополнительное условие \\(\\sum_i \\lambda_i f_i^k = f_0^k\\), получаем выражение для оценки дисперсии (ошибки) универсального кригинга:\\[\\color{red}{\\boxed{\\color{blue}{\\sigma_{UK} = \\E[Z^* - Z_0]^2 = \\sum_{}\\lambda_i\\gamma_{i0} - \\sum_k \\mu_k f_0^k}}}\\]","code":""},{"path":"gstat.html","id":"кросс-валидация","chapter":"Глава 1 Геостатистика","heading":"1.5.9 Кросс-валидация","text":"Значение переменной \\(Z(x)\\) оценивается в каждой точке \\(x_i\\) по данным в соседних точках \\(Z(x_j), ~ j \\neq \\) как если бы \\(Z(x_i)\\) было неизвестно.В каждой точке вычисляется оценка кригинга \\(Z_{-}^*\\) и соответствующая дисперсия кригинга \\(\\sigma_{Ki}^2\\). Поскольку значение \\(Z_i = Z(x_i)\\) известно, мы можем вычислить:Ошибку кригинга \\(E_i = Z_{-}^* - Z_i\\)Стандартизированную ошибку \\(e_i = E_i / \\sigma_{Ki}\\)Если \\(\\gamma(h)\\) — теоретическая вариограмма, то \\(E_i = Z_{-}^* - Z_i\\) — случайная величина с м.о. = \\(0\\) и дисперсией \\(\\sigma_{K_i}^2\\), а \\(e_i\\) имеет м.о. = \\(0\\) и дисперсию, равную \\(1\\).Стандартно анализируются следующие карты и графики:Карта стандартизированных ошибок \\(e_i\\). Стационарность ошибок, отсутствие эффекта пропорциональности.Карта стандартизированных ошибок \\(e_i\\). Стационарность ошибок, отсутствие эффекта пропорциональности.Гистограмма стандартизированных ошибок \\(e_i\\). Нормальность распределения, отсутствие аномалий.Гистограмма стандартизированных ошибок \\(e_i\\). Нормальность распределения, отсутствие аномалий.Диаграмма рассеяния \\((Z_{-}^*, Z_i)\\). Сглаживающий эффект, соответствие оценки и реального значения.Диаграмма рассеяния \\((Z_{-}^*, Z_i)\\). Сглаживающий эффект, соответствие оценки и реального значения.Диаграмма рассеяния \\((Z_{-}^*, e_i)\\). Независимость (ортогональность) оценки и ошибки.Диаграмма рассеяния \\((Z_{-}^*, e_i)\\). Независимость (ортогональность) оценки и ошибки.","code":""},{"path":"gstat.html","id":"вариограмма","chapter":"Глава 1 Геостатистика","heading":"1.5.10 Вариограмма","text":"Вариограмма (полувариограмма) дисперсия разности значений в точках как функция от их взаимного положения:\\[\\gamma (h) = \\gamma(x, x + h) = \\E\\big[Z(x + h)-Z(x)\\big]^2\\]Для \\(N\\) точек, разделенных вектором \\(\\mathbf{h}\\):\\[\\gamma(h) = \\frac{1}{2N(h)}\\sum_{=1}^{N(\\mathbf{h})} \\big[Z(x_i) - Z(x_i + h)\\big]^2\\]Свойства вариограммы:Вариограмма симметрична:\n\\[\\gamma(x) = \\gamma(-x)\\]Вариограмма симметрична:\\[\\gamma(x) = \\gamma(-x)\\]Вариограмма связана с дисперсией:\n\\[\\gamma(\\infty) = \\Var\\big[ Z(x) \\big]\\]Вариограмма связана с дисперсией:\\[\\gamma(\\infty) = \\Var\\big[ Z(x) \\big]\\]Вариограмма связана с ковариацией:\n\\[\\gamma(h) = \\Var\\big[ Z(x) \\big] - C(h)\\]Вариограмма связана с ковариацией:\\[\\gamma(h) = \\Var\\big[ Z(x) \\big] - C(h)\\]Вариограмма чувствительна к аномальным значениям (по причине второй степени)Вариограмма чувствительна к аномальным значениям (по причине второй степени)","code":""},{"path":"gstat.html","id":"модели-вариограмм","chapter":"Глава 1 Геостатистика","heading":"1.5.11 Модели вариограмм","text":"В уравнениях кригинга нельзя использовать эмпирическую вариограмму. Это связано с тем, что вывод этих уравнений опирается на предположение, что вариограмма представляет собой условно положительно определенную функцию. Поэтому следующим шагом после вычисления эмпирической вариограммы является подбор теоретической модели, которая наилучшим образом аппроксимирует эмпирическу функцию, и при этом отвечает требованию условной положительной определенности. Рассмотрим несколько распространенных на практике моделей.","code":""},{"path":"gstat.html","id":"сферическая-модель","chapter":"Глава 1 Геостатистика","heading":"1.5.12 Сферическая модель","text":"\\[\\gamma(h) = \\begin{cases}\n  c_0 + c\\Big[\\frac{3h}{2a} - \\frac{1}{2}\\big(\\frac{h}{}\\big)^3\\Big], & h \\leq ; \\\\\n  c_0 + c, & h > .\n\\end{cases}\\]\\[\\gamma() = \\Var\\big[Z(p)\\big] = c_0 + c\\]Данная модель достигает плато в точке \\(h = \\).","code":"\nn = 60\na = 40\nh = 0:n\n\ntab = tibble::tibble(\n  h = 0:60,\n  gamma = c(3 * (0:(a-1)) / (2 * a) - 0.5 * (0:(a-1) / a)^3, rep(1, n-a+1))\n)\n\nggplot() +\n  geom_line(tab, mapping = aes(h, gamma), size = 1, color = 'steelblue') +\n  geom_vline(xintercept = a, color = 'orangered') +\n  annotate(\"text\", x = a + 3, y = 0.5625, label = paste(\"a =\", a), color = 'orangered') + \n  theme_bw()"},{"path":"gstat.html","id":"экспоненциальная-модель","chapter":"Глава 1 Геостатистика","heading":"1.5.13 Экспоненциальная модель","text":"\\[\\gamma(h) = \\begin{cases}\n  0, & h = 0; \\\\\n  c_0 + (c-c_0)\\Big[1 - \\exp\\big(\\frac{-3h}{}\\big)\\Big], & h \\neq 0.\n\\end{cases}\\]\\[\\gamma() = \\Var\\big[Z(p)\\big] = c_0 + c\\]Данная модель достигает плато асимптотически.В точке \\(h = \\) достигается \\(95\\%\\) уровня плато.","code":"\ntab = tibble::tibble(\n  h = h,\n  gamma = 1 - exp(-3*h/a)\n)\n\npl = ggplot() +\n  geom_line(tab, mapping = aes(h, gamma), size = 1, color = 'steelblue') +\n  geom_vline(xintercept = a, color = 'orangered') +\n  annotate(\"text\", x = a + 3, y = 0.5625, label = paste(\"a =\", a), color = 'orangered') + \n  theme_bw()\n\n(pl)"},{"path":"gstat.html","id":"гауссова-модель","chapter":"Глава 1 Геостатистика","heading":"1.5.14 Гауссова модель","text":"\\[\\gamma(h) = c_0 + c\\Bigg[1 - \\exp\\bigg(\\frac{-3h^2}{^2}\\bigg)\\Bigg]\\]Данная модель достигает плато асимптотически.В точке \\(h = \\) достигается \\(95\\%\\) уровня плато.Отличительной чертой этой модели является ее гладкость: параболическое поведение вблизи нуля и асимптотическое приближение к плато.","code":"\ntab = tibble::tibble(\n  h = h,\n  gamma = 1 - exp(-3*h^2/a^2)\n)\n\npl = ggplot() +\n  geom_line(tab, mapping = aes(h, gamma), size = 1, color = 'steelblue') +\n  geom_vline(xintercept = a, color = 'orangered') +\n  annotate(\"text\", x = a + 3, y = 0.5625, label = paste(\"a =\", a), color = 'orangered') + \n  theme_bw()\n\n(pl)"},{"path":"gstat.html","id":"степенная-модель","chapter":"Глава 1 Геостатистика","heading":"1.5.15 Степенная модель","text":"\\[\\gamma(h) = \\begin{cases}\n  0, & h = 0; \\\\\n  c h^\\alpha, & h \\neq 0.\n\\end{cases}\\]Автокорреляция присутствует на всех расстояниях: \\(\\rightarrow \\infty\\)Предположение о стационарности второго порядка не выполняетсяКак правило, это означает наличие тренда в данных","code":"\ntab = tibble::tibble(\n  h = h,\n  gamma = h^1.5\n)\n\npl = ggplot() +\n  geom_line(tab, mapping = aes(h, gamma), size = 1, color = 'steelblue') +\n  theme_bw()\n\n(pl)"},{"path":"gstat.html","id":"эффект-самородка-модель-наггет","chapter":"Глава 1 Геостатистика","heading":"1.5.16 Эффект самородка (модель наггет)","text":"\\[\\gamma(h) = \\begin{cases}\n  0, & h = 0; \\\\\n  c_0, & h \\neq 0.\n\\end{cases}, ~ c_0 = C(0)\\]Наличие у данных вариограммы типа наггет означает отсутствие пространственной корреляции.Возможные причины:\nАбсолютно случайное распределение\nМелкомасштабная вариабельность (меньше, чем расстояние между измерениями)\nОшибки в измерениях\nОшибки в координатах точек\nАбсолютно случайное распределениеМелкомасштабная вариабельность (меньше, чем расстояние между измерениями)Ошибки в измеренияхОшибки в координатах точек","code":"\ntab = tibble::tibble(\n  gamma = rep(1, n+1),\n  h = h\n)\n\nggplot() +\n  geom_line(tab, mapping = aes(h, gamma), size = 1, color = 'steelblue') +\n  geom_point(data = data.frame(x = 0, y = 1), mapping = aes(x, y), shape=21,\n             colour = 'steelblue', fill = 'white', size = 3, stroke = 1.5) +\n  annotate('point', x = 0, y = 0, color = 'steelblue', size = 4) +\n  theme_bw()"},{"path":"gstat.html","id":"диаграмма-рассеяния-с-лагом","chapter":"Глава 1 Геостатистика","heading":"1.5.17 Диаграмма рассеяния с лагом","text":"Lagged scatterplot — вариант диаграммы рассеяния, на котором показываются значения в точках, расстояние между которыми попадает в заданный интервал","code":"\noptions(scipen = 999)\n\ncities = st_read(\"data/Italy_Cities.gpkg\")\n## Reading layer `Italy_Cities' from data source \n##   `/Users/tsamsonov/GitHub/r-spatstat-course/data/Italy_Cities.gpkg' \n##   using driver `GPKG'\n## Simple feature collection with 8 features and 37 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: 368910.4 ymin: 4930119 xmax: 686026 ymax: 5115936\n## Projected CRS: WGS 84 / UTM zone 32N\n\nrainfall = read_table2(\"data/Rainfall.dat\") %>% \n  st_as_sf(coords = c('x', 'y'), \n           crs = st_crs(cities),\n           remove = FALSE)\n\nhscat(rain_24~1, data = rainfall, 1000 * c(0, 10, 20, 50, 100), pch = 19)"},{"path":"gstat.html","id":"вариограммное-облако","chapter":"Глава 1 Геостатистика","heading":"1.5.18 Вариограммное облако","text":"Квадрат разности значений как функция от расстояния между точками","code":"\nvarcl = variogram(rain_24~1, data=rainfall, cutoff = 150000, cloud=TRUE)\n\nggplot(varcl) +\n  geom_point(aes(dist, gamma), alpha = 0.5, size = 2, color = 'steelblue') +\n  ylab('semivariance') +\n  theme_bw()"},{"path":"gstat.html","id":"эмпирическая-вариограмма","chapter":"Глава 1 Геостатистика","heading":"1.5.19 Эмпирическая вариограмма","text":"Эмпирическая вариограмма рассчитывается путем разбения вариограммного облака на интервалы расстояний — лаги — и подсчета среднего значения \\(\\gamma\\) в каждом лаге по следующей формуле:\\[\\hat{\\gamma} = \\frac{1}{2N_h} \\sum_{x_i - x_j \\approx h} \\big[z(x_i) - z(x_j)\\big]^2\\]Оставив только вариограмму, получим:Поскольку вариограмма есть дисперсия разности значений, ее рост при увеличении расстояния можно оценить также по увеличению размера «ящика» на диаграмме размаха \\(\\sqrt\\gamma\\):","code":"\nwidth = 10000\nintervals = width * 0:15\n\nvargr = variogram(rain_24~1, data=rainfall, cutoff = 150000, width = width)\nggplot() +\n  geom_line(vargr, mapping = aes(dist, gamma)) +\n  geom_point(vargr, mapping = aes(dist, gamma, size = np)) +\n  scale_size(range = c(1, 5)) +\n  theme_bw()\nvarcl = varcl %>% \n  mutate(sqgamma = sqrt(gamma),\n         lag = cut(dist, breaks = intervals, labels = 0.001 * (intervals[-1] - 0.5*width)))\n\nggplot(varcl) +\n  geom_boxplot(aes(lag, sqrt(gamma)), outlier.alpha = 0.1)"},{"path":"gstat.html","id":"вариокарта","chapter":"Глава 1 Геостатистика","heading":"1.5.20 Вариокарта","text":"Вариокарта (variogram map, variomap) представляет вариограмму как функцию приращений координат:\n\\[\\hat{\\gamma} (\\Delta x, \\Delta y) = \\frac{1}{2N_{\\substack{\\Delta x\\\\ \\Delta y}}} \\sum_{\\substack{\\Delta x_{ij} \\approx \\Delta x\\\\ \\Delta y_{ij} \\approx \\Delta y}} \\big[z(p_i) - z(p_j)\\big]^2\\]Вариокарта используется для выявления пространственной анизотропии. Профиль по линии из центра к краю вариокарты даст эмпирическую вариограмму","code":"\nvarmp = variogram(rain_24~1, data=rainfall, cutoff = 150000, width = width, map = TRUE)[['map']]"},{"path":"gstat.html","id":"приближение-теоретической-модели","chapter":"Глава 1 Геостатистика","heading":"1.5.21 Приближение теоретической модели","text":"Приближение (fitting) модели вариограммы предполагает:Выбор теоретической моделиПодбор параметров модели: эффект самородка (nugget), радиус корреляции и плато.Дана вариограмма семейства \\(\\gamma (h; \\mathbf{b})\\), где \\(\\mathbf{b} = (b_1, ..., b_k)\\) — вектор из \\(k\\) параметров модели. Параметры \\(\\mathbf{b}\\) подбираются таким образом, чтобы минимизировать следующий функционал:\\[Q(\\mathbf{b}) = \\sum_{l=1}^{L} w_l \\big[\\hat{\\gamma}(h_l) - \\gamma (h; \\mathbf{b})\\big]^2,\\]где \\(\\big\\{\\hat{\\gamma} (h_l): l = 1,...,L\\big\\}\\) — значения эмпирической вариограммы для \\(L\\) лагов, вычисленные по \\(N(h_l)\\) векторам.Веса \\(w_l\\) обычно выбираются исходя из отношения \\(w_l = N(h_l) / |h_l|\\), чтобы придать большее значение коротким расстояниям и лагам с хорошей оценкой.Минимизация функционала осуществляется итеративно:Процесс начинается с некоторого предположения \\(\\mathbf{b}^{(0)}\\)На шаге \\(s\\) функция \\(Q\\) аппроксимируется в виде квадратичной формы \\(Q(\\mathbf{b}^{(s)}) \\approx \\sum_{=1}^k \\sum_{j=1}^k \\delta_{ij} b_i b_j\\) путем разложения в ряд Тейлора вокруг точки \\(\\mathbf{b}^{(s)}\\).Новая точка минимума \\(\\mathbf{b}^{(s+1)}\\) находится как минимум квадратичной формы (этот минимум один).Шаги 2-3 повторяются до тех пор, пока значение \\(Q\\) не станет меньше заданного порога.Сравним результат ручного и автоматического приближения вариограммы:","code":"\nvarmd = fit.variogram(vargr, model = vgm(psill = 215, model = 'Sph', range = 120000, nugget = 15))\n\nh0 = lag * 0:(varmd[2, 'range']/lag)\nh1 = lag * (varmd[2, 'range']/lag + 1):(cutoff/lag) \n\ntab2 = tibble::tibble(\n  h = c(h0, h1),\n  gamma = c(varmd[1, 'psill'] + (varmd[2, 'psill'] * (3 * h0 / (2 * varmd[2, 'range']) - 0.5 * (h0 / varmd[2, 'range'])^3)), rep(varmd[1, 'psill'] + varmd[2, 'psill'], length(h1))),\n  fit = 'automatic'\n)\n\ntab = bind_rows(tab1, tab2)\n\nggplot() +\n  geom_line(vargr, mapping = aes(dist, gamma)) +\n  geom_point(vargr, mapping = aes(dist, gamma), size = 2) +\n  scale_size(range = c(1, 5)) +\n  geom_line(tab, mapping = aes(h, gamma, color = fit), size = 1) +\n  xlab('lag') + ylab('gamma') +\n  ggtitle('Сферическая модель') +\n  theme_bw()"},{"path":"gstat.html","id":"обычный-кригинг-1","chapter":"Глава 1 Геостатистика","heading":"1.5.22 Обычный кригинг","text":"Рассмотрим данные по температуре:","code":"\nbox = st_bbox(rainfall)\nenvelope = box[c(1,3,2,4)]\n\npx_grid = st_as_stars(box, dx = 2000, dy = 2000)\n\nggplot() + \n  geom_sf(data = rainfall, color = 'red') +\n  geom_sf(data = st_as_sf(px_grid), size = 0.5, fill = NA)"},{"path":"gstat.html","id":"обычный-кригинг-2","chapter":"Глава 1 Геостатистика","heading":"1.5.23 Обычный кригинг","text":"Визуализируем найденную вариограмму и вариокарту:Проинтерполируем, используя приближенную модель вариограммы:","code":"\nplot(vargr, model = varmd)\nplot(varmp)\n(px_grid = krige(rain_24~1, rainfall, px_grid, model = varmd))\n## [using ordinary kriging]\n## stars object with 2 dimensions and 2 attributes\n## attribute(s):\n##                  Min.   1st Qu.   Median     Mean  3rd Qu.      Max.\n## var1.pred  -0.4091735  7.707571 18.83325 21.50978 32.07393  67.26636\n## var1.var   30.9929191 45.435980 52.71968 58.67491 65.48474 186.22488\n## dimension(s):\n##   from  to  offset delta                refsys x/y\n## x    1 213  332239  2000 WGS 84 / UTM zone 32N [x]\n## y    1  99 5121556 -2000 WGS 84 / UTM zone 32N [y]"},{"path":"gstat.html","id":"оценка-и-дисперсия-кригинга","chapter":"Глава 1 Геостатистика","heading":"1.5.24 Оценка и дисперсия кригинга","text":"Дисперсия кригинга высока там, где мало данных.","code":"\nrain_colors = colorRampPalette(c(\"white\", \"dodgerblue\", \"dodgerblue4\"))\nrain_levels = seq(0,80,by=10)\nrain_ncolors = length(rain_levels)-1\n\nerr_colors = colorRampPalette(c(\"white\", \"coral\", \"violetred\"))\nerr_levels = seq(0, 180, by = 20)\nerr_ncolors = length(err_levels) - 1\n\ncont = st_contour(px_grid['var1.pred'], breaks = rain_levels, contour_lines = TRUE)\nconterr = st_contour(px_grid['var1.var'], breaks = err_levels, contour_lines = TRUE)\n\nggplot() +\n  geom_stars(data = cut(px_grid['var1.pred'], breaks = rain_levels)) +\n  scale_fill_manual(name = 'мм',\n                    values = rain_colors(rain_ncolors),\n                    labels = paste(rain_levels[-rain_ncolors-1], '-', rain_levels[-1]),\n                    drop = FALSE) +\n  coord_sf(crs = st_crs(rainfall)) +\n  geom_sf(data = cont, color = 'black', size = 0.2) +\n  geom_sf(data = rainfall, color = 'black', size = 0.3) +\n  theme_bw()\n\nggplot() +\n  geom_stars(data = cut(px_grid['var1.var'], breaks = err_levels)) +\n  scale_fill_manual(name = 'мм',\n                    values = err_colors(err_ncolors),\n                    labels = paste(err_levels[-err_ncolors-1], '-', err_levels[-1]),\n                    drop = FALSE) +\n  coord_sf(crs = st_crs(rainfall)) +\n  geom_sf(data = conterr, color = 'black', size = 0.2) +\n  geom_sf(data = rainfall, color = 'black', size = 0.3) +\n  theme_bw()"},{"path":"gstat.html","id":"кросс-валидация-1","chapter":"Глава 1 Геостатистика","heading":"1.5.25 Кросс-валидация","text":"Для выполнения кросс-валидации воспользуемся функцией krige.cv:Cтандартизированные ошибки в стационарном случае должны быть распределены нормально:Ошибки должны быть независимы от значений:Облако рассеяния оценки относительно истинных значений должно быть компактным:Пространственная картина стандартизированных ошибок должна быть гомогенной:","code":"\ncvl = krige.cv(rain_24~1, rainfall, varmd) %>% \n  st_as_sf() %>% \n  mutate(sterr = residual / sqrt(var1.var))\n\nhead(cvl %>% st_set_geometry(NULL), 10)\n##    var1.pred var1.var observed     residual      zscore fold       sterr\n## 1   5.743730 34.84033      6.0   0.25627005  0.04341669    1  0.04341669\n## 2  11.137129 60.24070     10.0  -1.13712865 -0.14650910    2 -0.14650910\n## 3   6.929502 47.22732      7.0   0.07049833  0.01025846    3  0.01025846\n## 4  23.252858 48.06354      1.0 -22.25285758 -3.20979954    4 -3.20979954\n## 5  15.655167 56.76258      1.0 -14.65516724 -1.94517957    5 -1.94517957\n## 6  11.794241 44.03055      1.0 -10.79424095 -1.62672846    6 -1.62672846\n## 7  11.325378 62.65261      0.1 -11.22537769 -1.41818009    7 -1.41818009\n## 8  28.421330 75.24988      0.2 -28.22133030 -3.25330355    8 -3.25330355\n## 9   2.340115 58.30350      1.0  -1.34011550 -0.17550719    9 -0.17550719\n## 10  3.489972 62.96551      0.2  -3.28997242 -0.41461106   10 -0.41461106\nggplot(cvl, aes(x = sterr)) +\n  geom_histogram(aes(y = stat(density)), fill = 'grey', color = 'black', size = 0.1) +\n  geom_density(fill = 'olivedrab', alpha = 0.5) +\n  theme_bw()\nggplot(cvl, aes(x = var1.pred, sterr)) +\n  geom_point(alpha = 0.8) +\n  geom_smooth(method = 'lm') +\n  theme_bw()\n\ncor.test(~ sterr + var1.pred, data = cvl)\n## \n##  Pearson's product-moment correlation\n## \n## data:  sterr and var1.pred\n## t = -0.084465, df = 253, p-value = 0.9328\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.1280692  0.1176091\n## sample estimates:\n##          cor \n## -0.005310204\nggplot(cvl, aes(x = var1.pred, observed)) +\n  geom_point(alpha = 0.8) +\n  geom_smooth(method = 'lm') +\n  theme_bw()\n\n# Диагностика модели линейной регрессии\nsummary(lm(observed ~ var1.pred, cvl))\n## \n## Call:\n## lm(formula = observed ~ var1.pred, data = cvl)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -47.812  -3.914  -0.505   3.227  32.685 \n## \n## Coefficients:\n##             Estimate Std. Error t value            Pr(>|t|)    \n## (Intercept) -0.03375    0.93884  -0.036               0.971    \n## var1.pred    1.00020    0.03919  25.519 <0.0000000000000002 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 8.405 on 253 degrees of freedom\n## Multiple R-squared:  0.7202, Adjusted R-squared:  0.7191 \n## F-statistic: 651.2 on 1 and 253 DF,  p-value: < 0.00000000000000022\nlibrary(akima)\n\ncoords = st_coordinates(rainfall)\ncoords_grid = st_coordinates(px_grid)\n\npx_grid = px_grid %>% \n  mutate(sterr = interpp(x = coords[,1],\n                         y = coords[,2],\n                         z = cvl$sterr, \n                         xo = coords_grid[,1],\n                         yo = coords_grid[,2],\n                         linear = FALSE,\n                         extrap = TRUE)$z)\n\nsterr_levels = seq(-8,8,2)\nsterr_ncolors = length(sterr_levels)-1\nsterr_colors = colorRampPalette(c('blue', 'white', 'red'))\n\nsterrcont = st_contour(px_grid['sterr'], breaks = sterr_levels, contour_lines = TRUE)\n\nggplot() +\n  geom_stars(data = cut(px_grid['sterr'], breaks = sterr_levels)) +\n  scale_fill_manual(name = 'мм',\n                    values = sterr_colors(sterr_ncolors),\n                    labels = paste(sterr_levels[-sterr_ncolors-1], '-', sterr_levels[-1]),\n                    drop = FALSE) +\n  coord_sf(crs = st_crs(rainfall)) +\n  geom_sf(data = sterrcont, color = 'black', size = 0.2) +\n  geom_sf(data = rainfall, color = 'black', size = 0.3) +\n  theme_bw()"},{"path":"gstat.html","id":"geostat_qt","chapter":"Глава 1 Геостатистика","heading":"1.6 Контрольные вопросы и упражнения","text":"","code":""},{"path":"gstat.html","id":"geostat_q","chapter":"Глава 1 Геостатистика","heading":"1.6.1 Вопросы","text":"Дайте определения случайной величины, случайного процесса, сечения случайного процесса, реализации случайного процесса. Проиллюстрируйте их примерами.Сформулируйте определение основных моментов пространственного случайного процесса: математического ожидания, дисперсии и ковариации. Поясните суть этих понятий на конкретных примерах.Дайте определение стационарности в строгом смысле слова и стационарности второго порядка.При каком условии случайная функция является изотропной?Что такое эргодичность? Пояснить суть этого свойства на наглядном примере.Сформулируйте суть кригинга как метода интерполяции данных. В чем его сходства и отличия в сравнении с методом радиальных базисных функций?Что из себя представляет среднеквадратическая ошибка, минимизируемая в методе кригинга?Почему среднеквадратическую ошибку можно считать равной дисперсии в методе простого кригинга?Что характеризует дисперсия кригинга?Напишите систему уравнений простого кригинга и выражение для дисперсии простого кригинга.Дайте определение стационарности приращений. С какой целью вводится данный тип стационарных процессов?Какие условия должны быть выполнены для того чтобы ковариацию можно было заменить вариограммой в уравнениях кригинга?Напишите систему уравнений обычного кригинга и выражение для дисперсии обычного кригинга. Чем эти уравнения отличаются от уравнений простого кригинга?Изложите модель универсального кригинга в математической и словесной форме.Что такое базисные функции и ковариаты? Какую роль они выполняют в методе универсального кригинга?Сформулируйте условия универсальности, от которых ведет свое название метод универсального кригинга.Напишите систему уравнений универсального кригинга и выражение для дисперсии универсального кригинга.Перечислите стандартный набор действий, применяемых в рамках выполнения процедуры кросс-валидации результатов кригинга.Что такое вариограмма? Дайте математическое и словесное определение.Перечислите свойства вариограммы.Назовите основные модели вариограммы. Какая модель свидетельствует о наличии пространственного тренда в данных?Чем эмпирическая вариаограмма отличается от теоретической (модели)?Сформулируйте принципы построения и назначение основных диагностических графиков вариографии: диаграммы рассеяния с лагом, вариограммного облака, эмпирической вариограммы, вариокарты.Объясните, каким образом можно получить приближение (подгонку) теоретической модели вариограммы под эмпирические данные.Расскажите об основных возможностях пакета gstat. Перечислите функции этого пакета, которые используются для вариографии и интерполяции методом кригинга.","code":""},{"path":"gstat.html","id":"geostat_t","chapter":"Глава 1 Геостатистика","heading":"1.6.2 Упражнения","text":"Загрузите данные дрейфующих буев ARGO на акваторию Северной Атлантики за 30 января 2010 года. Постройте поля распределения солености и температуры методом обычного кригинга с размером ячейки 50 км. Подберите подходящую модель вариограммы. Выполните визуализацию оценки кригинга и дисперсии кригинга средствами ggplot2. Произведите кросс-валидацию полученных результатов.\n\nПодсказка: перед построением сетки интерполяции странсформируйте данные в проекцию Меркатора. Чтобы полученное поле распределения покрывало только акваторию, маскируйте полученный растр с использованием слоя ocean из набора данных Natural Earth. Перед выполнением маскирования преобразуйте мультиполигон в обычные полигоны (в противном случае маскирование отработает некорректно).\nЗагрузите данные дрейфующих буев ARGO на акваторию Северной Атлантики за 30 января 2010 года. Постройте поля распределения солености и температуры методом обычного кригинга с размером ячейки 50 км. Подберите подходящую модель вариограммы. Выполните визуализацию оценки кригинга и дисперсии кригинга средствами ggplot2. Произведите кросс-валидацию полученных результатов.Подсказка: перед построением сетки интерполяции странсформируйте данные в проекцию Меркатора. Чтобы полученное поле распределения покрывало только акваторию, маскируйте полученный растр с использованием слоя ocean из набора данных Natural Earth. Перед выполнением маскирования преобразуйте мультиполигон в обычные полигоны (в противном случае маскирование отработает некорректно).","code":""},{"path":"spreg.html","id":"spreg","chapter":"Глава 2 Пространственная регрессия","heading":"Глава 2 Пространственная регрессия","text":"","code":""},{"path":"spreg.html","id":"review","chapter":"Глава 2 Пространственная регрессия","heading":"2.1 Краткий обзор","text":"Для просмотра презентации щелкните на ней один раз левой кнопкой мыши и листайте, используя кнопки на клавиатуре:Презентацию можно открыть в отдельном окне или вкладке браузере. Для этого щелкните по ней правой кнопкой мыши и выберите соответствующую команду.","code":""},{"path":"spreg.html","id":"autocorrelation_intro","chapter":"Глава 2 Пространственная регрессия","heading":"2.2 Введение","text":"В данном модуле мы приступим к исследованию связей в географическом пространстве. Понятие пространственной автокорреляции является математическим отражением первого закона географии: все связано со всем, но близкорасположенные объекты связаны сильнее. Различные коэффициенты пространственной автокорреляции, такие как индекс Морана (Moran’s ) позволяют охарактеризовать силу этой связи с точки зрения математической статистики. Однако для их вычисления необходимо формализовать понятия географической близости, или географического соседства. В настоящем модуле рассматриваются различные подходы к решению данной проблемы.Пространственные случайные процессы часть характеризуются следующими свойствами:Пространственная зависимость (spatial dependence) — наличие автокорреляции наблюдений. Выражается в невыполнении условия независимости остатков линейной регрессии. Устраняется посредством пространственной регрессии (spatial regression).Пространственная зависимость (spatial dependence) — наличие автокорреляции наблюдений. Выражается в невыполнении условия независимости остатков линейной регрессии. Устраняется посредством пространственной регрессии (spatial regression).Пространственная гетерогенность (spatial heterogeneity) — нестационарность процессов, порождающих наблюдаемую переменную. Выражается в неэффективности постоянных коэффициентов линейной регрессии. Устраняется постредством географически взвешенной регрессии (geographically weighted regression).Пространственная гетерогенность (spatial heterogeneity) — нестационарность процессов, порождающих наблюдаемую переменную. Выражается в неэффективности постоянных коэффициентов линейной регрессии. Устраняется постредством географически взвешенной регрессии (geographically weighted regression).Общепринятого определения пространственной автокоорреляции (ПА) не существует. Одно из наиболее удачных определений гласит следующее:Для множества \\(S\\), состоящего из \\(n\\) географических единиц, пространственная автокорреляция есть соотношение между переменной, наблюдаемой в каждой из \\(n\\) единиц и мерой географической близости, определенной для всех \\(n(n − 1)\\) пар единиц из \\(S\\) (Hubert et al., 1981)1Анализ ПА, как правило, осуществляется по жестко зафиксированной сетке (lattice) учетных единиц, в качестве которых могут выступать как площади, так и точки. Но, строго говоря, пространственная статистика в конечном счете любую единицу будет интерпретировать как точку.Конечной целью исследований ПА является построение статистической модели зависимости значения показателя в каждой единице от значений в соседних единицах и (опционально) неких факторов. Наличие статистически значимой ПА говорит о влиянии процессов, обуславливающих кластеризацию значений в соседних территориальных единицах. И пока эти механизмы не установлены, модель ПА дает инструмент их статистического моделирования. Добавление известных факторов в модель может улучшить точность моделирования.","code":""},{"path":"spreg.html","id":"линейная-регрессия","chapter":"Глава 2 Пространственная регрессия","heading":"2.3 Линейная регрессия","text":"Пусть дан вектор \\(\\mathbf{y} = \\{y_1, y_2, ... y_n\\}\\) измерений зависимой переменной, а также матрица \\(\\mathbf{X} = \\{x_{ij}\\}\\) размером \\(n \\times m\\), состоящая из значений \\(m\\) независимых переменных для \\(n\\) измерений. В этом случае модель линейной регрессии может быть записана как\\[\\mathbf{y} = \\mathbf{X} \\boldsymbol\\beta + \\boldsymbol\\epsilon,\\]где:\\(\\boldsymbol\\beta\\) — вектор коэффициентов регрессии;\\(\\boldsymbol\\beta\\) — вектор коэффициентов регрессии;\\(\\boldsymbol\\epsilon\\) — вектор случайных ошибок, независимо распределенных относительно среднего значения в нуле.\\(\\boldsymbol\\epsilon\\) — вектор случайных ошибок, независимо распределенных относительно среднего значения в нуле.Многомерное нормальное распределение (МНР) \\(k\\)-мерного случайного вектора \\(\\mathbf{X} = (X_1, ..., X_k)^T\\) обозначается как:\\[\\mathbf{X}\\ \\sim \\mathcal{N}_k(\\boldsymbol\\mu,\\, \\boldsymbol\\Sigma)\\]МНР определяется двумя параметрами:математическое ожидание ( \\(k\\)-мерный вектор):\\[\\boldsymbol\\mu = \\operatorname{E}[\\mathbf{X}] = [ \\operatorname{E}[X_1], \\operatorname{E}[X_2], \\ldots, \\operatorname{E}[X_k]]^{\\rm T}\\]ковариационная матрица (размером \\(k \\times k\\)):\\[\\boldsymbol\\Sigma = \\operatorname{E} [(\\mathbf{X} - \\boldsymbol\\mu)( \\mathbf{X} - \\boldsymbol \\mu)^{\\rm T}] =  [ \\operatorname{Cov}[X_i, X_j]; 1 \\le ,j \\le k ]\\]\nВещественнозначный случайный вектор \\(\\mathbf{X} = (X_1, ..., X_k)^T\\) называется стандартным нормальным случайным вектором, если все его компоненты \\(X_n\\) независимы друг от друга и подчиняются стандартному случаю нормального закона распределения с нулевым математическим ожиданием и единичной дисперсией для всех \\(n\\):\\[X_n \\sim \\mathcal{N}(0, 1)\\]В модели линейной регрессии:\\[\\boldsymbol\\epsilon \\sim \\mathcal{N}_k(0, \\sigma^2 \\mathbf{}),\\]где \\(\\) — единичная матрица размером \\(k \\times k\\).Если данные получены измерениями по пространству, остатки регрессии могут демонстрировать пространственную ассоциацию (зависимость), как правило свидетельствующую о наличии дополнительных неучтённых факторов. Это означает, что обычная модель регрессии недостаточно хорошо объясняет зависимость.В качестве примера приведем модель, в которой процент домохозяйств, находящихся во владении, моделируется как переменная зависимая от уровня безработицы [Fotheringam, Brunsdon, Charlton, 2002]:\nРис. 2.1: Процент домохозяйств, находящихся во владении\n\nРис. 2.2: Уровень безработицы\nОбычная линейная регрессия показывает хорошую согласованность между этими параметрами:\nРис. 2.3: Зависимость процента домохозяйств во владении от уровня безработицы\nОднако остатки регрессии демонстрируют явную пространственную зависимость, что говорит о том, что построенная модель неадекватно описывает исследуемую закономерность:\nРис. 2.4: Остатки линейной регрессии\nЧтобы моделировать подобную зависимость остатков, необходим более широкий класс моделей:\\[\\boldsymbol\\epsilon \\sim \\mathcal{N}_k(0, \\mathbf{C}),\\]где \\(\\mathbf{C}\\) — любая допустимая ковариационная матрица.Данная модель решает проблему независимости остатков, однако порождает две других проблемы:Если зависимость остатков имеет пространственный характер (ассоциированы остатки в территориально близких локациях), то матрица \\(\\mathbf{C}\\) характер этой зависимости не отражает в явном виде.Если зависимость остатков имеет пространственный характер (ассоциированы остатки в территориально близких локациях), то матрица \\(\\mathbf{C}\\) характер этой зависимости не отражает в явном виде.Вектор коэффициентов регрессии \\(\\boldsymbol\\beta\\) может быть получен путем минимизации \\(\\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta\\) путем решения \\(\\beta = \\big(\\mathbf{X}^T \\mathbf{CX} \\big)^{-1} \\mathbf{X}^T \\mathbf{X y}\\). Однако это требует знания ковариационной матрицы, которая обычно неизвестна. Поэтому как \\(\\mathbf{C}\\), так и \\(\\boldsymbol\\beta\\) калибруются по выборке.Вектор коэффициентов регрессии \\(\\boldsymbol\\beta\\) может быть получен путем минимизации \\(\\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta\\) путем решения \\(\\beta = \\big(\\mathbf{X}^T \\mathbf{CX} \\big)^{-1} \\mathbf{X}^T \\mathbf{X y}\\). Однако это требует знания ковариационной матрицы, которая обычно неизвестна. Поэтому как \\(\\mathbf{C}\\), так и \\(\\boldsymbol\\beta\\) калибруются по выборке.","code":""},{"path":"spreg.html","id":"пространственная-регрессия","chapter":"Глава 2 Пространственная регрессия","heading":"2.4 Пространственная регрессия","text":"Для того чтобы учесть пространственную автокорреляцию остатков, в модель линейной регрессии добавляется компонента пространственной авторегрессии (spatial autoregression), которая моделирует пространстенный лаг:\\[\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\color{red}{\\rho\\mathbf{Wy}} +  \\mathbf{\\epsilon},\\]\\(\\rho\\) — коэффициент регрессии, отражающий степень пространственной автокорреляции\\(\\rho\\) — коэффициент регрессии, отражающий степень пространственной автокорреляции\\(\\mathbf{W}\\) — матрица пространственных весов\\(\\mathbf{W}\\) — матрица пространственных весовПолученная модель называется моделью пространственной регрессии (spatial regression).Для получения коэффициентов \\(\\boldsymbol\\beta\\) и \\(\\rho\\) выполним ряд преобразований:\\[\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\rho\\mathbf{Wy} +  \\mathbf{\\epsilon}\\\\\n\\mathbf{y} - \\rho\\mathbf{Wy} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon}\\\\\n(\\mathbf{} - \\rho\\mathbf{W})\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon}\\]Предполагая, что матрица \\((\\mathbf{} - \\rho\\mathbf{W})\\) инвертируема, получаем систему уравнений пространственной регрессии:\\[\\color{red}{\\boxed{\\color{blue}{\\mathbf{y} = (\\mathbf{} - \\rho\\mathbf{W})^{-1}\\mathbf{X}\\mathbf{\\beta} + (\\mathbf{} - \\rho\\mathbf{W})^{-1}\\mathbf{\\epsilon}}}}\\]Данная модель идентична обычной регрессии \\(\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon}\\), но в ней независимые переменные и ошибки линейно трансформированы умножением на \\((\\mathbf{} - \\rho\\mathbf{W})^{-1}\\).\\[\\mathbf{y} = (\\mathbf{} - \\rho\\mathbf{W})^{-1}\\mathbf{X}\\mathbf{\\beta} + (\\mathbf{} - \\rho\\mathbf{W})^{-1}\\mathbf{\\epsilon}\\]Трансформированная ошибка модели будет иметь ковариационную матрицу\\[\\mathbf{C} = \\sigma^2 \\Big[\\big(\\mathbf{} - \\rho \\mathbf{W}\\big)^{-1}\\Big]^T (\\mathbf{} - \\rho\\mathbf{W})^{-1}\\]Если ковариационная матрица функционально зависит от параметра \\(\\rho\\), то она отражает пространственную структуру автокорреляции ошибок.Если ковариационная матрица функционально зависит от параметра \\(\\rho\\), то она отражает пространственную структуру автокорреляции ошибок.Ковариационная матрица должна быть положительно определенной. Для полученного выражения это будет выполняться в случае если \\(|\\rho| \\leq 1\\) (Griffith, 1988).Ковариационная матрица должна быть положительно определенной. Для полученного выражения это будет выполняться в случае если \\(|\\rho| \\leq 1\\) (Griffith, 1988).\\[\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\rho\\mathbf{Wy} +  \\mathbf{\\epsilon}\\]Для нахождения коэффициентов \\(\\boldsymbol\\beta\\) и \\(\\rho\\) используется минимизация квадрата случайной компоненты, которую можно представить как \\(\\mathbf{\\epsilon} = \\mathbf{y} - \\mathbf{X} \\mathbf{\\beta} - \\rho\\mathbf{Wy}\\):\\[\\sum_i \\Bigg(y_i - \\sum_j \\beta_j x_{ij} - \\rho \\sum_j w_{ij} y_j \\Bigg)^2\\]Задача решается в 2 этапа:находится оптимальное значение \\(\\rho\\);находится оптимальное значение \\(\\boldsymbol\\beta\\) путем подстановки в вышеуказанное выражение.Модель пространственной регрессии может быть использована для осуществления пространственной фильтрации — убирания автокорреляционной составляющей. Для этого необходимо авторегрессионную компоненту (пространственный лаг) перенести в левую часть уравнения:\\[\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\rho\\mathbf{Wy} + \\mathbf{\\epsilon}\\\\\n\\mathbf{y}^* = \\mathbf{y} - \\rho\\mathbf{Wy} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon}\\]Данная модель представляет собой стандартную (непространственную) регрессию для независимой переменной \\(\\mathbf{y}^*\\), в которой пространственная корреляция убрана (подвергнута фильтрации).Пространственная фильтрация бывает полезна, когда наблюдается несоответствие масштаба наблюдений и масштаба процесса.Пространственная фильтрация бывает полезна, когда наблюдается несоответствие масштаба наблюдений и масштаба процесса.Например, статистика по показателю, контролируемому на региональном уровне, собирается по муниципалитетам. В этом случае фильтрация позволяет подобрать параметры \\(\\mathbf{\\beta}\\), учитывающие наличие высокой пространственной автокорреляци.Например, статистика по показателю, контролируемому на региональном уровне, собирается по муниципалитетам. В этом случае фильтрация позволяет подобрать параметры \\(\\mathbf{\\beta}\\), учитывающие наличие высокой пространственной автокорреляци.","code":""},{"path":"spreg.html","id":"практический-анализ","chapter":"Глава 2 Пространственная регрессия","heading":"2.5 Практический анализ","text":"Как мы уже сказали, исследование ПА начинается с анализа географического соседства. То есть, для каждой анализируемой единицы мы должны определить соседние по отношению к ней единицы. Это не так-то просто,поскольку существует множество способов определить соседство.Перед выполнением анализа подключим необходимые библиотеки и визуализируем исходные данные, в качестве которых выступают границы муниципалитетов Кировской области.","code":"\nlibrary(sf)\nlibrary(spdep)  # оценка соседства, построение матрицы весов, индексы автокорреляции\nlibrary(spatialreg) # пространственная регрессия\nlibrary(lattice)\nlibrary(RANN)\nlibrary(RColorBrewer)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(GWmodel)\n\noptions(scipen = 999)\n\nreg_sf = st_read('data/Kirov.gpkg')\n## Reading layer `Kirov' from data source \n##   `/Users/tsamsonov/GitHub/r-spatstat-course/data/Kirov.gpkg' \n##   using driver `GPKG'\n## Simple feature collection with 40 features and 20 fields\n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -216808.3 ymin: 2896149 xmax: 227259 ymax: 3455774\n## Projected CRS: unnamed\nreg = st_geometry(reg_sf)\n\npar(mar = c(1, 1, 1, 1))\nplot(reg, border = \"gray50\")"},{"path":"spreg.html","id":"autocorrelation_neighbors","chapter":"Глава 2 Пространственная регрессия","heading":"2.6 Пространственное соседство","text":"В целом, можно выделить три большие группы методов:Соседи по смежностиСоседи по графуСоседи по метрикеСоседство по смежности основано на топологических отношениях между объектами и применяется при анализе данных, приуроченных к площадным единицам — например, сетке административно-территориального деления. Смежными считаются объекты, границы которых имеют общие точки. При этом возможно два варианта соседства: по правилу ферзя (QUEEN) и правилу ладьи (ROOK). В первом случае соседними будут считаться все пары территориальных единиц, имеющие хотя бы одну общую точку на границе, т.е. соприкасющиеся сторонами и/или углами. Соседство по правилу ладьи является более строгим, так как разрешает только наличие общих сторон вдоль границ, а точечные касания в расчет не берутся. Отличия правил иллюстрирует рисунок ниже.Поиск географических соседей по правилу ферзя и правилу ладьиСоседство по графу основано на отношениях объектов в триангуляции Делоне. В эту же категорию попадают всевозможные фильтрации триангуляции Делоне, которые удаляют из нее ребра, не удовлетворяющие заданным критериям. Более подбробно о них будет сказано ниже.Соседство по метрике основано на вычислении расстояний между объектами. Соседними по отношению к каждому объекту будут считаться либо \\(K\\) ближайших к нему объектов (соседи по количеству), либо все объекты, удаленные на растояние не далее \\(D_{max}\\) (соседи по расстоянию).Результатом анализа соседства является граф соседства(neighborhood graph), в котором сами объекты являются вершинами, а связи между ними — ребрами.Анализ географического соседства на языке R можно провести с помощью пакета spdep.Рассмотрим суть и принципы построения графов соседства на основе различных принципов.","code":""},{"path":"spreg.html","id":"autocorrelation_neighbors_contguity","chapter":"Глава 2 Пространственная регрессия","heading":"2.6.1 Соседи по смежности","text":"Список соседей по смежности можно получить с помощью функции poly2nb().Возвращаемый объект является классом типа nb. Для каждой единицы в нем содержится список номеров соседних по отношению к нему единиц. По умолчанию функция находит соседей по правилу ферзя:Для объектов типа nb в пакете spdep определена своя функция plot(), которая позволяет визуализировать граф соседства. Функция требует на вход координаты точек, в случае площадных единиц для этого используют центроиды площадей, которые можно получить функцией coordinates():Для определения соседей по правилу ладьи необходимо вызвать функцию poly2nb() с аргументом queen=FALSE. В нашем случае, правда, это даст тот же результат, поскольку в данных отсутствуют единицы, соприкасающиеся в одной лишь точке:Обратим внимание на то, что функция poly2nb() принимает на вход площадные объекты. Все помледующие методы определения соседства (по графу и по метрике) работают с точечными данными.","code":"\nnb_queen = poly2nb(reg) # Соседство по правилу ферзя\nnb_queen  # посмотрим сводную информацию\n## Neighbour list object:\n## Number of regions: 40 \n## Number of nonzero links: 174 \n## Percentage nonzero weights: 10.875 \n## Average number of links: 4.35\nclass(nb_queen)  # проверим тип объекта\n## [1] \"nb\"\ncoords = reg %>% \n  st_centroid() %>% \n  st_coordinates()\n\n# Теперь рисуем граф:\nplot(reg, border = \"gray50\")\nplot(nb_queen, coords, pch = 19, cex = 0.5, add = TRUE)\ntitle(main = \"Соседи по смежности (правило ферзя)\")\nnb_rook = poly2nb(reg, queen = FALSE) # Соседство по правилу ладьи\n\nplot(reg, border = \"grey70\")\nplot(nb_rook, coords, pch = 19, cex = 0.5, add = TRUE)\ntitle(main = \"Соседи по смежности (правило ладьи)\")"},{"path":"spreg.html","id":"autocorrelation_neighbors_graph","chapter":"Глава 2 Пространственная регрессия","heading":"2.6.2 Соседи по графу","text":"Данная группа методов определения соседства основана на построении триангуляции Делоне для точек исходных данных. Далее эта триангуляция может быть оставлена в неизменном виде, или быть подвержена процедуре фильтрации, которая удалит из нее ребра, не удовлетворяющие заданному критерию.Соседи по триангуляции Делоне без фильтрации могут быть получены с помощью функции tri2nb():Соседи по сфере влияния получаются путем фильтрации триангуляции Делоне. Для каждой вершины находится расстояние до ближайшего соседа \\(D_{min}\\) — это расстояние называется радиусом сферы влияния вершины. Остальные ребра триангуляции, инцидентные (примыкающие к) данной вершине, сохраняются только если их длина \\(D\\) превышает радиус ее сферы влияния не более чем вдвое: \\(D \\leq 2D_{min}\\). Рассуждая геометрически, можно сказать, что сферы радиусом \\(D_{min}\\), построенные в точке и ее соседях по триангуляции, должны пересекаться. Процесс фильтрации по сфере влияния иллюстрирует рисунок ниже.Поиск географических соседей по правилу сферы влиянияПоиск соседей по сфере влияния построен по аналогии с принципом сферы действия тяготения из небесной механики.Построение соседей по правилу сферы влияния осуществляется в 3 шага:определение соседей по триангуляции (функция tri2nb())фильтрация триангуляции по правилу сферы влияния (функция soi.graph())преобразование полученного объекта в класс nb(функция graph2nb())Соседи по графу Гэбриела получаются также путем фильтрации триангуляции Делоне. В каждом треугольнике ребро сохранятся только тогда, когда построенная на нем окружность не включает третью точку треугольника (Gabriel, Sokal, 1969)2. Данный метод проиллюстрирован рисунком ниже.Поиск географических соседей по графу ГэбриелаПоиск соседей по графу Гэбриела осуществляется в 2 шага:построение графа Гэбриела (функция gabrielneigh())преобразование полученного объекта в класс nb(функция graph2nb())Относительные соседи по графу получаются путем фильтрации триангуляции Делоне по следующему правилу: ребро \\(\\), соединяющее две вершины \\(p\\) и \\(q\\), будет удалено, если найдется третья вершина \\(r\\), такая что расстояния от нее до \\(p\\) и \\(q\\) (\\(B\\) и \\(C\\) соответственно) окажутся короче, чем \\(\\), то есть: \\(> B\\) \\(> C\\). Полученный граф носит название графа относительных соседей (relative neighborhood graph). Данный метод был предложен французским информатиком Готфридом Туассеном для выявления структуры множества точек, которая бы максимально соответствовала восприятию человеком формы этого множества (Toussaint, 1980)3.Поиск соседей по графу относительных соседей осуществляется в 2 шага:построение графа относительных соседей (функция relativeneigh())преобразование полученного объекта в класс nb(функция graph2nb())","code":"\nnb_tin = tri2nb(coords)\n\nplot(reg, border = \"grey70\")\nplot(nb_tin, coords, pch = 19, cex = 0.5, add = TRUE)\ntitle(main = \"Соседи по триангуляции Делоне\")\nnb_tin = soi.graph(nb_tin, coords) %>% graph2nb()\n\nplot(reg, border = \"grey70\")\nplot(nb_tin, coords, pch = 19, cex = 0.5, add = TRUE)\ntitle(main = \"Соседи по сфере влияния\")\nnb_gab = gabrielneigh(coords) %>% graph2nb()\n \nplot(reg, border = \"grey70\")\nplot(nb_gab, coords, pch = 19, cex = 0.5, add = TRUE)\ntitle(main = \"Соседи по графу Гэбриела\")\nnb_rel = relativeneigh(coords) %>% graph2nb()\n\nplot(reg, border = \"grey70\")\nplot(nb_rel, coords, pch = 19, cex = 0.5, add = TRUE)\ntitle(main = \"Относительные соседи по графу\")"},{"path":"spreg.html","id":"autocorrelation_neighbors_metrics","chapter":"Глава 2 Пространственная регрессия","heading":"2.6.3 Соседи по метрике","text":"Поиск соседей по метрике — наиболее простой способ определения соседства. Для его использования необходимо задать метрику (как правило, расстояние между точками), а также критерий фильтрации связей: по количеству (\\(k\\) ближайших) или по расстоянию (не ближе чем \\(d_1\\), но и не далее чем \\(d_2\\)).Поиск соседей по количеству осуществляется в 2 шага:построение списка соседей (функция knearneigh())преобразование полученного объекта в класс nb(функция knn2nb())Рассмотрим поиск по количеству на примере нескольких пороговых значений:Поиск соседей по расстоянию осуществляется средствами функции dnearneigh(), которая принимает 3 аргумента: координаты точек, минимальное \\(d_1\\) и максимальное \\(d_2\\) расстояние. Минимальное расстояние имеет смысл использовать чтобы избежать анализа совпадающих по положению объектов, или когда известен пространственный период явления, превышающий \\(d_1\\):Итак, мы рассмотрели различные принципы выявления географического соседства. После того, как определен сам факт соседства, необходимо оценить силу пространственной связи между всеми парами соседних единиц. Эта оценка производится путем построения матрицы пространственных весов(spatial weights matrix).","code":"\npar(mfrow = c(2,2),\n    mar = c(1,1,1,1))\nfor (i in 1:4){\n  nb_knn = knearneigh(coords, k = i) %>% knn2nb()\n  \n  plot(reg, border = \"grey70\")\n  plot(nb_knn, coords, pch = 19, cex = 0.5, add = TRUE)\n  title(main = paste(\"Ближайшие соседи (k = \", i, \")\", sep = ''))\n}\npar(mfrow = c(2,2),\n    mar = c(1,1,1,1))\nfor (d in 5:8) {\n  dnearnei = dnearneigh(coords, d1 = 0, d2 = 10000 * d)\n  \n  plot(reg, border = \"grey70\")\n  plot(dnearnei, coords, pch = 19, cex = 0.5, add = TRUE)\n  title(main = paste(\"Ближайшие соседи (d <=\", 10000 * d, \")\", sep = ''))\n}"},{"path":"spreg.html","id":"autocorrelation_weights","chapter":"Глава 2 Пространственная регрессия","heading":"2.7 Пространственные веса","text":"Пространственные веса характеризуют силу связи между единицами. Если единицы не являются соседними (по выбранному правилу), то пространственный вес их связи будет равен нулю. Во всех остальных случаях веса будут ненулевыми. Поскольку теоретически каждая единица может быть связана с любой другой единицей, распространена форма представления весов в виде матрицы \\(W\\) размером \\(N \\times N\\), где \\(N\\) – число единиц. На пересечении \\(\\)-й строки и \\(j\\)-го столбца матрицы располагается вес связи между \\(\\)-й и \\(j\\)-й единицей.Простейший вид матрицы \\(W\\) — бинарная. Если связь есть, то ее вес равен единице (1), если нет — нулю (0). Для построения бинарной матрицы нужно использовать функцию nb2listw() c параметром style=\"B\":Оказывается, что это на самом деле не матрица. Это объект с двумя слотами. В слоте weights содержатся веса, а в слоте neighbours — идентификаторы соседей:Дело в том, что матрица весов всегда получается разреженной. То есть, в основном она содержит нули. Это логично, поскольку у каждой точки как правило есть лишь ограниченное число соседей. При этом общее количество точек может быть достаточно большим: чем больше точек мы анализируем, тем больше будет нулей в матрице. Получается, что хранить матрицу как матрицу неэкономично. Более рационально для каждого объекта возвращать список весов, которые соответствуют его соседям. Что и делает функция nb2listw().Матрицу весов как правило визуализируют, поскольку она может содержать в себе довольно интересные паттерны. Для этого полученный список весов нужно превратить в матрицу с помощью функции listw2mat(). Далее использовать функцию levelplot из пакета lattice, которая раскрашивает ячейки матрицы или растрового набора данных:Более интересный результат дает нормированная матрица. В ней веса всех соседей нормируются на количество соседей. То есть, если у текущей точки 2 соседа, их веса будут равны 0.5. Если 3 соседа то 0.33, 4 — 0.25 и так далее. Взвешенная матрица позволяет отразить тот факт, что одна и та же территориальная единица может оказывать неодинаковое влияние на соседние единицы:Обратите внимание, что на этот раз цвета в матрице распределены асимметрично.Однако есть методы определения соседства, которые приведут также и к асимметричному виду самой матрицы, а не только значений. Например, при поиске соседей по количеству соседство и B вовсе не означает соседство B и . Проверим это на опыте:Полученная матрица весов дает искомую меру потенциальной пространственной связи (близости) между всеми парами территориальных единиц. Сопоставив эту меру со значениями показателя, зафиксированными в тех же единицах, можно получить статистическую оценку пространственной автокорреляции изучаемой величины.","code":"\nWbin = nb2listw(nb_queen, style = \"B\")\nWbin  # посмотрим, что за объект получается на выходе (listw)\n## Characteristics of weights list object:\n## Neighbour list object:\n## Number of regions: 40 \n## Number of nonzero links: 174 \n## Percentage nonzero weights: 10.875 \n## Average number of links: 4.35 \n## \n## Weights style: B \n## Weights constants summary:\n##    n   nn  S0  S1   S2\n## B 40 1600 174 348 3416\nWbin$neighbours\n## Neighbour list object:\n## Number of regions: 40 \n## Number of nonzero links: 174 \n## Percentage nonzero weights: 10.875 \n## Average number of links: 4.35\nWbin$weights\n## [[1]]\n## [1] 1 1 1 1 1\n## \n## [[2]]\n## [1] 1 1\n## \n## [[3]]\n## [1] 1 1 1 1 1 1\n## \n## [[4]]\n## [1] 1 1 1 1 1 1\n## \n## [[5]]\n## [1] 1 1 1 1\n## \n## [[6]]\n## [1] 1 1 1 1 1 1 1\n## \n## [[7]]\n## [1] 1\n## \n## [[8]]\n## [1] 1 1 1 1 1 1\n## \n## [[9]]\n## [1] 1 1 1 1 1 1\n## \n## [[10]]\n## [1] 1 1 1\n## \n## [[11]]\n## [1] 1 1 1\n## \n## [[12]]\n## [1] 1 1 1 1 1\n## \n## [[13]]\n## [1] 1 1 1 1 1\n## \n## [[14]]\n## [1] 1 1 1 1 1 1 1\n## \n## [[15]]\n## [1] 1 1 1 1 1 1\n## \n## [[16]]\n## [1] 1 1 1\n## \n## [[17]]\n## [1] 1 1\n## \n## [[18]]\n## [1] 1 1 1\n## \n## [[19]]\n## [1] 1 1 1 1\n## \n## [[20]]\n## [1] 1 1 1\n## \n## [[21]]\n## [1] 1 1 1 1 1 1\n## \n## [[22]]\n## [1] 1 1 1 1 1 1\n## \n## [[23]]\n## [1] 1 1 1 1\n## \n## [[24]]\n## [1] 1 1 1 1\n## \n## [[25]]\n## [1] 1 1 1 1 1 1\n## \n## [[26]]\n## [1] 1 1 1 1 1 1\n## \n## [[27]]\n## [1] 1 1 1 1\n## \n## [[28]]\n## [1] 1 1\n## \n## [[29]]\n## [1] 1 1\n## \n## [[30]]\n## [1] 1 1 1\n## \n## [[31]]\n## [1] 1 1 1 1 1 1\n## \n## [[32]]\n## [1] 1 1 1 1 1\n## \n## [[33]]\n## [1] 1 1 1 1 1\n## \n## [[34]]\n## [1] 1 1 1 1 1\n## \n## [[35]]\n## [1] 1 1 1\n## \n## [[36]]\n## [1] 1 1 1 1 1\n## \n## [[37]]\n## [1] 1 1 1 1 1\n## \n## [[38]]\n## [1] 1 1\n## \n## [[39]]\n## [1] 1 1 1 1\n## \n## [[40]]\n## [1] 1 1 1 1\n## \n## attr(,\"mode\")\n## [1] \"binary\"\n## attr(,\"B\")\n## [1] TRUE\nM = listw2mat(Wbin)\nlevelplot(M, main = \"Матрица весов (бинарная)\")\nWstand = nb2listw(nb_queen, style = \"W\")\nM = listw2mat(Wstand)\n\nramp = colorRampPalette(c(\"white\",\"red\"))\nlevels = 1 / 1:10  # шкала 1, 0.5, 0.33, 0.25 ... 0.1\nlevelplot(M, \n          main=\"Матрица весов (нормированная)\", \n          at = levels, \n          col.regions=ramp(10))\n# Ближайшие соседи (k = 1)\nnb_knn = knearneigh(coords, k = 1) %>% knn2nb()\n\nWstand = nb2listw(nb_knn, style = \"B\")\nM = listw2mat(Wstand)\nlevelplot(M, \n          main = \"Матрица весов (нормированная)\", \n          at = levels, \n          col.regions = ramp(10))"},{"path":"spreg.html","id":"autocorrelation_measures","chapter":"Глава 2 Пространственная регрессия","heading":"2.8 Пространственная автокорреляция","text":"Далее мы рассмотрим вычисление меры пространственной автокорреляции — индекса Морана, который дает оценку статистической зависимости между значением показателя в каждой локации (территориальной единице) и значениями в соседних локациях. Имея предположение о наличии пространственной автокорреляции, можно построить модель пространственной авторегрессии, которая дает фоновое распределение показателя по территориальным единицам, а также случайные остатки.На этом занятии мы кратко познакомимся со статистической оценкой пространственной автокорреляции, а также построением простейших авторегрессионных моделей.Мы будем использовать месячную статистику по случаям заболеваний верхних дыхательных путей в Кировской области за 2015 год (данные Росстата по районам, модифицированы автором для большей наглядности анализа).Вам предстоит выполнить следующую последовательность действий:Загрузить исходные данные (границы районов и таблицу со статистикой)Присоединить таблицу к пространственным даннымПостроить серию карт по месяцам для визуального анализа данныхВычислить матрицу пространственных весов \\(W\\)Вычислить -индекс Морана для численной оценки пространственной автокорреляцииПостроить диаграмму рассеяния Морана для визуальной оценки пространственной автокорреляцииПодобрать параметры модели пространственной авторегрессииПостроить карты реальных, модельных (fitted) значений и остатков (lag)Для начала построим серию карт чтобы оценить по ним наличие или отсутствие пространственной автокорреляции по месяцам:Данная серия карт показывает, что наиболее интересный для анализа месяц — февраль, в котором наблюдается рост заболеваемости, а также очевидно наличие пространственной автокорреляции с двумя очагами в центральных и северо-зпапдных районах области.Вычислим матрицу пространственных весов:","code":"\nmun_src = reg_sf\n\n# Чтение таблицы со статистикой\ntab = read_xlsx(\"data/Kirov.xlsx\", 1)\n\n# Соединение таблиц\nmun = mun_src %>% \n  left_join(tab, by = c(\"OBJECTID\" = \"N\")) %>% \n  pivot_longer(cols = 22:31,\n               names_to = 'month',\n               values_to = 'nsick') %>% \n  mutate(month = ordered(month, levels = c('Январь', 'Февраль', 'Март', \n                                           'Апрель', 'Май', 'Июнь', \n                                           'Июль', 'Август', 'Сентябрь', \n                                           'Октябрь', 'Ноябрь', 'Декабрь'))) %>% \n  st_set_geometry('geometry')\n\n# Построение серии карт\nramp = colorRampPalette(c(\"white\", \"orange\", \"red\"))\nlevels = seq(0, 10000, 1000)\nnclasses = length(levels) - 1\n\nggplot() +\n  geom_sf(mun, mapping = aes(geometry = geometry, \n                             fill = cut(nsick, levels))) +\n  scale_fill_manual(values = ramp(nclasses),\n                    labels = paste(levels[-nclasses-1], '-', levels[-1]),\n                    guide = guide_legend(reverse = TRUE),\n                    drop = FALSE) +\n  facet_wrap(~month)\n# Определение соседства (правило ферзя)\nnb_queen = poly2nb(mun_src)\n\n# Визиуализация графа соседства\ncoords = st_centroid(mun_src) %>% st_coordinates()\nplot(mun_src %>% st_geometry(), border = \"darkgray\")\nplot(nb_queen, coords, pch = 19, cex = 0.5, add = TRUE)\ntitle(main = \"Соседи по смежности (правило ферзя)\")\n\n# Вычисление весов (нормированная матрица)\nW = nb2listw(nb_queen)\n\n# Визуализация матрицы весов\nM = listw2mat(W)\n\nramp2 = colorRampPalette(c(\"white\",\"red\"))\nlevels2 = 1 / 1:10 # шкала 1, 0.5, 0.33, 0.25 ... 0.1\nlevelplot(M, \n          main = \"Матрица весов (нормированная)\", \n          at = levels2, \n          col.regions = ramp2(10))"},{"path":"spreg.html","id":"autocorrelation_moran","chapter":"Глава 2 Пространственная регрессия","heading":"2.8.1 Индекс Морана (Moran’s I)","text":"Анализ пространственной автокорреляции осуществляется, как правило, путем вычисления индекса Морана (Moran’s ), :\n\\[\n= \\frac{n \\sum^n_{=1} \\sum^n_{j=} w_{ij} (y_i - \\bar y)(y_j - \\bar y)}{ \\Big[\\sum^n_{=1} \\sum^n_{j=} w_{ij}\\Big] \\Big[\\sum^n_{=1} (y_i - \\bar y)^2\\Big]}\n\\]\nгде \\(n\\) — количество единиц, \\(w_{ij}\\) — вес пространственной связи между \\(\\)-й и \\(j\\)-й единицей, \\(y_i\\) — значение в \\(\\)-й единице, \\(\\bar y\\) — выборочное среднее по всем единицамОбратим внимание на то, что индекс Морана по сути и форме записи похож на линейный коэффициент корреляции Пирсона, в котором перебираются все пары соответствующих друг другу значений из рядов \\(X = \\{x_i\\}\\) и \\(Y = \\{y_i\\}\\):\\[\nr_{xy} = \\frac{\\sum_{=1}^{n}(x_i - \\bar x)(y_i - \\bar y)}{\\sqrt{\\sum_{=1}^{n}(x_i - \\bar x)^2} \\sqrt{\\sum_{=1}^{n}(y_i - \\bar y)^2}}\n\\]При вычислении индекса Морана происходит нечто подобное, но под соответствием понимается наличие соседства между \\(\\)-й и \\(j\\)-й территориальной единицей. Степень выраженности соседства задается весом \\(W_{ij}\\), который можно наблюдать в числителе формулы индекса Морана. Таким образом, пары территориальных единиц, для которых \\(w_{ij} = 0\\), не участвуют в вычислении индекса Морана.Индекс Морана для нормально распределенных данных лежит в диапазоне от -1 до 1:+1 означает детерминированную прямую зависимость — группировку схожих (низких\nили высоких) значений.0 означает абсолютно случайное распределение (CSR — complete spatial randomness)-1 означает детерминированную обратную зависимость — идеальное перемешивание\nнизких и высоких значений, напоминающее шахматную доскуДля вычисления индекса Морана следует использовать функцию moran.test(), которая дополнительно оценивает статистическую значимость полученного значения:Результаты теста включают в себя следующие значения:Moran statistic — полученный индекс МоранаExpectation — математическое ожидание индекса при нулевой гипотезе \\(E[]\\)Variance — дисперсия ожидаемого значения при нулевой гипотезе \\(D[]\\)Moran statistic standard deviate — \\(Z\\)-оценка вычисленного индекса Моранаp-value — \\(p\\)-значение вычисленного индекса МоранаЗдесь мы видим, что значение индекса Морана равно ~\\(0.52\\) (Moran statistic), то есть присутствует положительная пространственная автокорреляция. При этом вероятность того, что мы ошибаемся в наших выводах, и распределение на самом деле случайно - крайне мала и равна \\(2.408 \\times 10^{-7})\\) (p-value), то есть менее \\(0.0001\\%\\). Можно принимать гипотезу о наличии пространственной автокорреляции.Рассмотрим чуть подробнее, откуда берутся эти и остальные значения результатов теста, и как их правильно интерпретировать.Обычно для сравнения принимают предположение о том, что исследуемая величина распределена случайно. Это так называемая “нулевая” гипотеза. После того как мы вычислили индекс Морана по фактическим данным, можно вычислить его аналитически, приняв нулевую гипотезу. В этом случае математическое ожидание индекса \\(E[] = -1/(n-1)\\), где \\(n\\) - количество территориальных единиц. Также может быть вычислена и дисперсия индекса Морана \\(D[]\\) (в англоязычной литературе дисперсия обозначается \\(V[]\\)).Эти два параметра определяют функцию распределения индекса Морана при всевозможных случайных расстановках исследуемой величины по территориальным единицам. Грубо говоря, такое распределение получится, если мы извлечем все фактические данные, будем их случайным образом перемешивать между территориями и каждый раз вычислять индекс Морана, повторяя процедуру бесконечное число раз. Полученные индексы будут распределены нормально.Значимость фактического индекса Морана можно оценить путем его сравнения с ожидаемым значением индекса \\(E[]\\) и его стандартным отклонением \\(s = \\sqrt D\\) Для такой оценки используется \\(Z\\)-тест Фишера. \\(Z\\)-значение вычисляется по формуле: \\(Z = (- E[])/s\\)Эта величина говорит нам о том, на какое количество стандартных отклонений фактическое значение индекса Морана удалено от ожидаемого значения. Чем сильнее оно удалено — тем менее вероятно, что фактическое распределение случайно. Какова же эта вероятность? Каждому значению \\(Z\\)-score соответствует \\(p\\)-значение (p-value). P-value — это вероятность появления значений, удаленных от мат. ожидания далее чем \\(Z\\)-score.Например, при:\\(Z < -1.96\\) или \\(Z > +1.96\\) значение \\(p < 0.05\\)\\(Z < -2.58\\) или \\(Z > +2.58\\) значение \\(p < 0.01\\)Это означает, что вероятность того, что фактическое значение индекса Морана могло бы появиться на основе случайно распределенных данных с вероятностью \\(5\\%\\) и \\(1\\%\\) соответственно. Чем меньше \\(p\\), тем менее вероятно, что распределение случайно. Говорят, что \\(p\\) — это вероятность сделать ошибку первого рода, т.е. отвергнуть нулевую гипотезу, в то время как она на самом деле является истинной.","code":"\n# Выбираем данные за февраль\nfeb = mun %>% \n  dplyr::filter(month == 'Февраль')\n\n# Вычисление индекса (тест) Морана\nmoran.test(feb$nsick, W)\n## \n##  Moran I test under randomisation\n## \n## data:  feb$nsick  \n## weights: W    \n## \n## Moran I statistic standard deviate = 5.0335, p-value = 0.0000002408\n## alternative hypothesis: greater\n## sample estimates:\n## Moran I statistic       Expectation          Variance \n##        0.52118132       -0.02564103        0.01180194"},{"path":"spreg.html","id":"autocorrelation_permutation","chapter":"Глава 2 Пространственная регрессия","heading":"2.8.2 Перестановочный тест Морана","text":"Графически вышеприведенные рассуждения можно иллюстрировать с помощью перестановочного теста (permutation test). Для этого значения исследуемой нами величины перемешиваются между территориальными единицами и далее строится гистограмма распределения. Перестановочный тест выполняется с помощью функции moran.mc() с параметром nsim =, задающим число перестановок:","code":"\n(sim = moran.mc(feb$nsick, listw = W, nsim = 10000))\n## \n##  Monte-Carlo simulation of Moran I\n## \n## data:  feb$nsick \n## weights: W  \n## number of simulations + 1: 10001 \n## \n## statistic = 0.52118, observed rank = 10001, p-value = 0.00009999\n## alternative hypothesis: greater\n\n# Построим гистограмму по вычисленным индексам:\nhist(sim$res,\n     freq = TRUE,\n     breaks = 20, \n     xlim = c(-1,1),\n     main = \"Перестановочный тест Морана\", \n     xlab = \"Случайный индекс Морана\",\n     ylab = \"Частота появления\",\n     col = \"steelblue\")\n\n# Нанесем фактическое значение\nabline(v = sim$statistic, col = \"red\")"},{"path":"spreg.html","id":"autocorrelation_moranscatter","chapter":"Глава 2 Пространственная регрессия","heading":"2.8.3 Диаграмма рассеяния Морана","text":"Наконец, еще одним графическим инструментом оценки пространственной автокорреляции является диаграмма рассеяния Морана. По оси \\(X\\) откладывается значение в каждой территориальной единице, в по оси \\(Y\\) — ее пространственный лаг, который представляет собой средневзвешенное значение по всем ее соседям:На диаграмме рассеяния Морана линиями отмечаются средние значения по обеим осям, а наклонной линией представляется линейная регрессия этих значений, при этом тангенс угла наклона прямой равен значению индекса Морана. Поскольку в данном случае распределение явно не случайно, можно приступать к его моделированию.","code":"\nmoran.plot(feb$nsick, W)"},{"path":"spreg.html","id":"autocorrelation_autoregression","chapter":"Глава 2 Пространственная регрессия","heading":"2.8.4 Пространственная авторегрессия","text":"Поиск уравнения пространственной регрессии и его авторегрессионной составляющей может быть выполнен посредством функций spautolm() и lagsarlm() из пакета spatialreg. В качестве первого параметра каждой из этих функций выступает формула, которая работает по следующему принципу:Y ~ 1 позволит найти коэффициенты \\(\\rho\\) и остатки \\(\\epsilon\\) пространственной авторегрессии вида \\(\\mathbf{y} = \\mu + \\rho\\mathbf{Wy} + \\mathbf{\\epsilon}\\), где \\(\\mu\\) — константа.Y ~ 1 позволит найти коэффициенты \\(\\rho\\) и остатки \\(\\epsilon\\) пространственной авторегрессии вида \\(\\mathbf{y} = \\mu + \\rho\\mathbf{Wy} + \\mathbf{\\epsilon}\\), где \\(\\mu\\) — константа.Y ~ X позволит найти коэффициенты \\(\\beta\\), \\(\\rho\\) и остатки \\(\\epsilon\\) пространственной регрессии вида \\[\\mathbf{y} = \\mu + \\mathbf{X} \\mathbf{\\beta} + \\rho\\mathbf{Wy} +  \\mathbf{\\epsilon}.\\]Y ~ X позволит найти коэффициенты \\(\\beta\\), \\(\\rho\\) и остатки \\(\\epsilon\\) пространственной регрессии вида \\[\\mathbf{y} = \\mu + \\mathbf{X} \\mathbf{\\beta} + \\rho\\mathbf{Wy} +  \\mathbf{\\epsilon}.\\]Y ~ X - 1 позволит найти коэффициенты \\(\\beta\\), \\(\\rho\\) и остатки \\(\\epsilon\\) пространственной регрессии вида \\[\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\rho\\mathbf{Wy} + \\mathbf{\\epsilon}\\] без свободного члена \\(\\mu\\)Y ~ X - 1 позволит найти коэффициенты \\(\\beta\\), \\(\\rho\\) и остатки \\(\\epsilon\\) пространственной регрессии вида \\[\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\rho\\mathbf{Wy} + \\mathbf{\\epsilon}\\] без свободного члена \\(\\mu\\)Расшифровываются параметры модели следующим образом:Intercept = \\(\\mu\\),rho = \\(\\rho\\)Извлекаем модельные значения \\(Z\\) и записываем в таблицу","code":"\nmodel = lagsarlm(nsick ~ 1, data = feb, listw = W)\nmodel\n## \n## Call:\n## lagsarlm(formula = nsick ~ 1, data = feb, listw = W)\n## Type: lag \n## \n## Coefficients:\n##         rho (Intercept) \n##    0.704957 1223.330336 \n## \n## Log likelihood: -346.2344\n# Извлекаем результаты пространственной авторегрессии\nfeb_spreg = feb |>  \n  mutate(fitted = model$fitted.values,\n         residual = model$residuals) |>  \n  pivot_longer(all_of(c(\"nsick\", \"fitted\", \"residual\")), # TODO: cannot find sick as usual!\n               names_to = 'type',\n               values_to = 'value') |>  \n  st_set_geometry('geometry')\n\n# Построение серии карт\nramp = colorRampPalette(c('steelblue3', 'white', 'orange', 'violetred'))\nlevels = seq(-3000, 10000, 1000)\nnclasses = length(levels) - 1\n\n# Сравниваем исходные данные, модельные и остатки\nggplot() +\n  geom_sf(feb_spreg, mapping = aes(geometry = geometry, fill = cut(value, levels))) +\n  scale_fill_manual(values = ramp(nclasses),\n                    labels = paste(levels[-nclasses-1], '-', levels[-1]),\n                    guide = guide_legend(reverse = TRUE),\n                    drop = FALSE) +\n  facet_wrap(~type)"},{"path":"spreg.html","id":"questions_tasks_spreg","chapter":"Глава 2 Пространственная регрессия","heading":"2.9 Контрольные вопросы и упражнения","text":"","code":""},{"path":"spreg.html","id":"questions_spreg","chapter":"Глава 2 Пространственная регрессия","heading":"2.9.1 Вопросы","text":"Дайте определение пространственной автокорреляции.В чем выражается пространственная зависимость и пространственная гетерогенность?Сформулируйте задачу и напишите уравнение пространственной авторегрессии. Чем обусловлена необходимость в использовании такой модели?Что такое пространственный лаг?В чем заключается процедура пространственной фильтрации?Перечислете основные методы определения географического соседства.Чем отличается соседство по методу ферзя и соседство по методу ладьи?Какие методы установления соседства напрямую неприменимы к точечным данным?Что понимается под сферой влияния в соответствующем методе установления географического соседства? По аналогии с каким принципом небесной механики построен поиск соседей на основе этого принципа?Что объединяет все методы поиска соседей по графу? На основе какой структуры данных они работают?Сформулируйте принцип фильтрации ребер графа, с помощью которого получается граф Гэбриела.Что понимается под относительным соседством по графу?Назовите два основных метода определения соседства по метрике.Перечислите основные функции пакетов R, которые можно использовать для установления географического соседства.С помощью каких структур данных осуществляется моделирование соседства в R?Дайте определение пространственного веса. Что харктеризует эта величина?Назовите основные методы вычисления пространственных весов.Какая структура данных используется для хранения информации о пространственных весах? Какими свойствами она обладает?Перечислите индексы, которые могут использоваться для оценки пространственной автокорреляции.Напишите формулу для вычисления индекса Морана. Какие значения он может принимать и как их следует интерпретировать? Аналогом какого статистического коэффициента является данный индекс?Изложите алгоритм выполнения и назначение перестановочного теста Морана.Что из себя представляет диаграмма рассеяния Морана и в каких задачах они может быть востребована?Для каких целей используется локальный анализ пространственной автокорреляции (LISA)? Какой коэффициент можно использовать для квантификации локальных оценок автокорреляции? Как он связан с индексом Морана?","code":""},{"path":"spreg.html","id":"tasks_spreg","chapter":"Глава 2 Пространственная регрессия","heading":"2.9.2 Упражнения","text":"Выполните анализ пространственной зависимости (автокорреляции) для данных Росстата по урожайности зерновых культур по муниципалитетам Ростовской, Волгоградской и Саратовской областей. Для этого загрузите границы муниципалитетов и статистику за 2012 и 2013 гг. В процессе выполнения анализа необходимо:\nВизуализировать исходные данные.\nОтфильтровать из данных Волгоград, Ростов-на-Дону и Саратов, поскольку для них статистика отсутствует.\nПостроить граф соседства методом ферзя и визуализировать его.\nВычислить матрицу пространственных весов и визуализировать ее.\nВыполнить тест Морана для данных 2013 г. (далее тоже везде 2013 год).\nПодобрать параметры модели пространственной авторегрессии.\nВизуализировать результаты моделирования (исходные данные, модельные данные, остатки)\nВизуализировать исходные данные.Отфильтровать из данных Волгоград, Ростов-на-Дону и Саратов, поскольку для них статистика отсутствует.Построить граф соседства методом ферзя и визуализировать его.Вычислить матрицу пространственных весов и визуализировать ее.Выполнить тест Морана для данных 2013 г. (далее тоже везде 2013 год).Подобрать параметры модели пространственной авторегрессии.Визуализировать результаты моделирования (исходные данные, модельные данные, остатки)","code":""},{"path":"gwr.html","id":"gwr","chapter":"Глава 3 Географически взвешенная регрессия","heading":"Глава 3 Географически взвешенная регрессия","text":"","code":"\nlibrary(sf)\nlibrary(RColorBrewer)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(GWmodel)"},{"path":"gwr.html","id":"review","chapter":"Глава 3 Географически взвешенная регрессия","heading":"3.1 Краткий обзор","text":"Для просмотра презентации щелкните на ней один раз левой кнопкой мыши и листайте, используя кнопки на клавиатуре:Презентацию можно открыть в отдельном окне или вкладке браузере. Для этого щелкните по ней правой кнопкой мыши и выберите соответствующую команду.","code":""},{"path":"gwr.html","id":"autocorrelation_gwr","chapter":"Глава 3 Географически взвешенная регрессия","heading":"3.2 Географически взвешенная регрессия (GWR)","text":"В стандартной модели линейной регрессии параметры модели предполагаются постоянными:\\[\\mathbf{y} = \\mathbf{X} \\boldsymbol\\beta + \\boldsymbol\\epsilon,\\]Для \\(\\)-й локации решению выглядит следующим образом:\\[y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ... + \\beta_m x_{mi} + \\epsilon_i\\]Коэффициенты находятся методом наименьших квадратов:\\[\\mathbf{\\beta}' = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y}\\]Такой подход, однако не учитывает того, что характер зависимости между переменными может меняться по пространству.В географически взвешенной регрессионной модели веса определяются для каждой локации:\\[y_i = \\beta_{0i} + \\beta_{1i} x_{1i} + \\beta_{2i} x_{2i} + ... + \\beta_{mi} x_{mi} + \\epsilon_i\\]В этом случае область оценки параметров \\(\\mathbf{\\beta}\\) ограничивается некой окрестностью точки \\(\\). Математически это достигается применением весовых коэффициентов для данных независимых переменных:\\[\\mathbf{\\beta}'() = (\\mathbf{X}^T \\color{blue}{\\mathbf{W}()}\\mathbf{X})^{-1} \\mathbf{X}^T \\color{blue}{\\mathbf{W}()} \\mathbf{Y},\\]где \\(\\mathbf{W}()\\) есть матрица весов для точки \\(\\). Коэффициенты матрицы подбираются таким образом, что близкие локации получают более высокий вес.Матрица \\(\\mathbf{W}()\\) имеет размер \\(n \\times n\\), где \\(n\\) — число точек наблюдений:\\[\\mathbf{W}() = \\begin{bmatrix}\n    w_{i1} & 0 & 0 & \\dots  & 0 \\\\\n    0 & w_{i2} & 0 & \\dots  & 0 \\\\\n    0 & 0 & w_{i3} & \\dots  & 0 \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    0 & 0 & 0 & \\dots  & w_{}\n\\end{bmatrix},\\]где \\(w_{ik}\\) есть вес, который имеет точка \\(k\\) при локальной оценке параметров в точке \\(\\).","code":""},{"path":"gwr.html","id":"весовые-функции","chapter":"Глава 3 Географически взвешенная регрессия","heading":"3.2.1 Весовые функции","text":"Весовая функция должна быть убывающей. Существует множество вариантов таких функций, но наиболее часто используются гауссоподобные варианты:\nРис. 2.2: Весовая функция\nВ случае фиксированной весовой функции окрестность всегда имеет фиксированный размер:\\[w_{ij} = \\operatorname{exp}\\{-\\frac{1}{2} (d_{ij}/h)^2\\},\\]где \\(d_{ij}\\) есть расстояние, \\(h\\) — полоса пропускания.\nРис. 2.3: Фиксированная весовая функция\nВ случае адаптивной весовой функции окрестность ограничивается \\(N\\) ближайшими точками. За пределами этой окрестности веса принимаются равными нулю:\nРис. 2.4: Адаптивная весовая функция\nПолоса пропускания \\(h\\) обладает следующими особенностями:малая полоса пропускания приводит к большой дисперсии локальных оценок;большая полоса пропускания приводит к смещенности оценки;при \\(h \\rightarrow \\infty\\) локальная модель приближается к глобальной регрессии;при \\(h \\rightarrow 0\\) локальная модель «сворачивается» вокруг данных.","code":""},{"path":"gwr.html","id":"практический-анализ-1","chapter":"Глава 3 Географически взвешенная регрессия","heading":"3.2.2 Практический анализ","text":"В качестве примера проанализируем каким образом цена жилья зависит от количества комнат на примере данных по стоимости недвижимости в Бостоне, доступных на данном сайте, и выгруженных с североамериканского информационного портала недвижимости padmapper.com:Для того чтобы оценить пространственую неравномерность реакции стоимости жилья на увеличение количества комнат, построим модель географически взвешенной регрессии:Как видно, модель GWR наглядно показывает наличие пространственной гетерогенности (неоднороности) в распределении показателя. Четко видны районы (в основном цеентральные, но также и часть окраинных), где стоимость жилья резко возрастает при увеличении количества комнат.","code":"\nrealest = read_delim(url('https://www.jefftk.com/apartment_prices/apts-1542637382.txt'),\n                 delim = ' ',\n                 col_names = c('price', 'rooms', 'id', 'lon', 'lat')) %>%\n  st_as_sf(coords = c('lon', 'lat'), crs = 4326) %>%\n  st_transform(3395)\n\n# tmap_mode('view')\ntm_shape(realest) +\n  tm_bubbles(col = 'price',\n             size = 'rooms',\n             style = 'fixed',\n             breaks = c(0, 1000, 2000, 3000, 4000, 5000, 10000, max(realest$price)),\n             scale = 0.25,\n             palette = colorRampPalette(c('steelblue4', 'orange', 'darkred'))(7),\n             alpha = 0.8) +\n  tm_view(symbol.size.fixed = TRUE)\nsamples = realest %>% dplyr::sample_n(1000) %>% as('Spatial')\n\n(gwr_res = gwr.basic(price ~ rooms, data = samples, bw = 1000, kernel = 'gaussian'))\n\ntm_shape(gwr_res$SDF) +\n  tm_bubbles(col = 'rooms', # это не количество комнат, а коэффициент регрессии\n             style = 'quantile',\n             scale = 0.3,\n             palette = 'Reds',\n             alpha = 0.5) +\n  tm_view(symbol.size.fixed = TRUE)"},{"path":"gwr.html","id":"questions_tasks_gwr","chapter":"Глава 3 Географически взвешенная регрессия","heading":"3.3 Контрольные вопросы и упражнения","text":"","code":""},{"path":"gwr.html","id":"questions_gwr","chapter":"Глава 3 Географически взвешенная регрессия","heading":"3.3.1 Вопросы","text":"Сформулируйте задачу и напишите уравнение географически взвешенной регрессии. Чем обусловлена необходимость в использовании такой модели?Определите назначение весовой функции в методе географически взвешенной регрессии. Каков ее основной параметр?","code":""},{"path":"gwr.html","id":"tasks_gwr","chapter":"Глава 3 Географически взвешенная регрессия","heading":"3.3.2 Упражнения","text":"","code":""},{"path":"points.html","id":"points","chapter":"Глава 4 Точечные паттерны","heading":"Глава 4 Точечные паттерны","text":"","code":""},{"path":"points.html","id":"review","chapter":"Глава 4 Точечные паттерны","heading":"4.1 Краткий обзор","text":"Для просмотра презентации щелкните на ней один раз левой кнопкой мыши и листайте, используя кнопки на клавиатуре:Презентацию можно открыть в отдельном окне или вкладке браузере. Для этого щелкните по ней правой кнопкой мыши и выберите соответствующую команду.","code":""},{"path":"points.html","id":"точечный-паттерн","chapter":"Глава 4 Точечные паттерны","heading":"4.2 Точечный паттерн","text":"Точечный паттерн (point pattern) представляет собой множество точек в \\(\\mathbb{R}^2\\), обозначаемое малой жирной буквой:\\[\\mathbf{x} = \\{x_1, x_2,...x_n\\}\\]\n- Количество точек \\(n = n(\\mathbf{x})\\) может быть любым неотрицательным числомМножество является неупорядоченным (индексы чисто условны)Множество является неупорядоченным (индексы чисто условны)Допускаются дубликаты (совпадающие точки), однако большинство методов рассчитаны на то, что дубликатов в множестве нет.Допускаются дубликаты (совпадающие точки), однако большинство методов рассчитаны на то, что дубликатов в множестве нет.Если \\(\\mathbf{x}\\) представляет точечный паттерн и \\(B\\) — это некий регион, то \\(\\mathbf{x} \\cap B\\) есть подмножество \\(\\mathbf{x}\\), состоящее из точек, попадающих в \\(B\\):Baddeley et. al., 2016В данном случае количество точек, попадающих в \\(B\\), равняется \\(n = n(\\mathbf{x} \\cap B)\\)","code":""},{"path":"points.html","id":"точечные-процессы","chapter":"Глава 4 Точечные паттерны","heading":"4.3 Точечные процессы","text":"__ Точечным процессом__ называется случайный процесс, реализациями которого являются точечные паттерныКонечный точечный процесс (finite point process) — это точечный процесс, каждая реализация которого представляет собой точечный паттерн с конечным числом точек. При этом для любой области \\(B\\) количество точек \\(\\mathbf{x} \\cap B\\) представляет собой случайную величину с определимыми параметрами.Локально конечный точечный процесс имеет конечное число точек в любой ограниченной области \\(B\\) (более мягкое утверждение). Реализацией такого процесса является локально конечный точечный паттерн.","code":""},{"path":"points.html","id":"равномерно-случайные-точки","chapter":"Глава 4 Точечные паттерны","heading":"4.3.1 Равномерно случайные точки","text":"Простейшим точечныйм процессом является процесс \\(U = (U_1, U_2)\\), каждая реализация которого включает одну точку \\(u = (u_1, u_2)\\).Случайная точка будет равномерно распределена в пространственной облсти \\(W\\), если ее координаты \\((U_1, U_2)\\) имеют совместную плотность распределения, которая постоянна в пределах \\(W\\) и равна нулю за ее пределами.Поскольку интеграл плотности распределения равен 1, величина постоянной будет равна \\(1/|W|\\):\\[f(u_1, u_2) = \\begin{cases}\n  1/|W|,~\\text{если}~(u_1, u_2) \\W\\\\\n  0, ~\\text{в противном случае}\n\\end{cases}\\]Если \\(B\\) представляет собой тестовую область в \\(W\\), то вероятность того, что точка \\(U\\) попадет в \\(B\\), будет равна:\\[\\mathbb{P}\\{U \\B \\} = \\int_B f(u_1, u_2) du_1 du_2 = \\\\ = \\frac{1}{|W|} \\int_B 1 du_1 du_2 = \\frac{|B|}{|W|}\\]\n- эта вероятность равна доле площади \\(B\\) в \\(W\\)вероятность зависит только от площади, и не зависит от положения и формы области \\(B\\)","code":""},{"path":"points.html","id":"биномиальный-точечный-процесс","chapter":"Глава 4 Точечные паттерны","heading":"4.3.2 Биномиальный точечный процесс","text":"Биномиальным называется точечный процесс \\(\\mathbf{X} = \\{X_1,..., X_n\\}\\), реализации которого содержат \\(n\\) точек.Baddeley et. al., 2016Чтобы точки были распределены равномерно по пространству, необходимо выполнение двух условий:\\(X_1,...,X_n\\) представляют собой независимые случайные величиныКаждая из этих величин равномерно распределена в пределах \\(W\\).Если \\(B\\) представляет собой тестовую область, то количество \\(n(\\mathbf{X} \\cap B)\\) случайных точек, попавших в \\(B\\), будет равняться количеству индексов \\(\\) таких, что \\(X_i \\B\\).Чтобы определить вероятностное распределение \\(n(\\mathbf{X} \\cap B)\\), рассмотрим эту величину как количество успешных исходов в \\(n\\) независимых испытаниях. Будем считать «успехом» исход, при котором случайная точка \\(X_i\\) попадает в \\(B\\).Если испытания независимы и равномерно распределены, то вероятность успеха равна \\(p = |B| / |W|\\) и величина \\(n(\\mathbf{X} \\cap B)\\) имеет биномиальный закон распределения:\\[\\mathbb{P}\\{n(\\mathbf{X} \\cap B) = k\\} =\n  \\left(\n    \\begin{array}{c}\n      n \\\\\n      k\n    \\end{array}\n  \\right) p^k (1-p)^{n-k}\\color{grey}{, k = 0, 1, ..., n}\\]Биномиальный коэффициент равен числу сочетаний из \\(n\\) по \\(k\\) (без учета порядка):\n\\[\\left(\n    \\begin{array}{c}\n      n \\\\\n      k\n    \\end{array}\n  \\right) = \\frac{n!}{(n-k)!~k!}\\]","code":""},{"path":"points.html","id":"пуассоновский-процесс","chapter":"Глава 4 Точечные паттерны","heading":"4.3.3 Пуассоновский процесс","text":"Однородный пуассоновский точечный процесс (homogeneous Poisson point process), или абсолютная пространственная случайность (complete spatial randomness — CSR) характеризуется следующими свойствами:гомогенность: размещение точек не имеет пространственных закономерностейнезависимость: результат реализации процесса в одной области не оказывает влияние на результат реализации в других областяхBaddeley et. al., 2016Однородность (гомогенность) означает, что ожидаемое количество точек, попадающих в регион \\(B\\) должно быть пропорционально его площади:\\[\\operatorname E[n(\\mathbf{X} \\cap B)] = \\lambda |B|\\]\nПараметр \\(\\lambda\\) представляет собой среднее количество точек на единицу площади — интенсивность точечного процесса.В отличие от биномиального процесса, полностью случайный (Пуассоновский) процесс характеризуется случайным количеством точек.Пространственная независимость означает, что количества точек в двух неперекрывающихся областях \\(\\) и \\(B\\) являются независимыми случайными переменными:\\[n(\\mathbf{X} \\cap ) \\\\sim n(\\mathbf{X} \\cap B),~\\cap B = \\emptyset\\]\n> Для биномиального процесса условие независимости не выполняется, поскольку известно общее количество точек. Если в область \\(\\) попало \\(k\\) точек, то в область \\(B\\) не может попасть более чем \\(n-k\\) точек, что нарушает условие независимости распределений.Предположение о независимости выполняется для любых непересекающихся регионов \\(\\) и \\(B\\) и для любого числа этих регионовОдним из следствий независимости является тот факт, что количество точек, подсчитанное по регулярной сетке квадратов, также даст совокупность независимых величин (для любого размера сетки):Baddeley et. al., 2016Упорядоченность (orderliness): при стремлении площади области к нулю, вероятность нахождения в этой области более одной точки, деленная на площадь, также стремится к нулюBaddeley et. al., 2016Если реализации отвечают условию независимости и вероятность нахождения более одной точки в бесконечно малом квадрате пренебрежимо мала, то случайную величину \\(n(\\mathbf{X} \\cap B)\\) можно рассматривать как количество «успехов» в большом числе независимых испытаний, каждое из которых имеет малую вероятность успеха.Это означает, что \\(n(\\mathbf{X} \\cap B)\\) подчиняется распределению Пуассона, которое характеризует частоту редких событий.В соответствии с этим законом, вероятность получить \\(k\\) редких событий равна\\[\\mathbb{P}\\{N = k\\} = e^{-\\mu} \\frac{\\mu^k}{k!}\\color{grey}{,~k = 0, 1, 2, ...}\\]Величина \\(\\mu\\) представляет собой математическое ожидание распределения Пуассона.Дисперсия распределения Пуассона равна его математическому ожиданиюПоскольку, как мы показали ранее, ожидаемое количество точек в регионе \\(B\\) равняется \\(\\operatorname E[n(\\mathbf{X} \\cap B)] = \\lambda |B|\\), можно сделать вывод, что случайная величина \\(n(\\mathbf{X} \\cap B)\\) имеет распределение Пуассона с математическим ожиданием:\\[\\mu = \\lambda |B|\\]Пуассоновский процесс определяется следующими параметрами:однородность: количество \\(n(\\mathbf{B} \\cap B)\\) случайных точек, попадающих в выборочную область \\(B\\) характеризуется мат. ожиданием \\(\\mathbb{E}n(\\mathbf{X} \\cap B) = \\lambda |B|\\);однородность: количество \\(n(\\mathbf{B} \\cap B)\\) случайных точек, попадающих в выборочную область \\(B\\) характеризуется мат. ожиданием \\(\\mathbb{E}n(\\mathbf{X} \\cap B) = \\lambda |B|\\);независимость: для неперекрывающихся выборочных областей \\(B_1, B_2, ..., B_k\\), количества \\(n(\\mathbf{X} \\cap B_1), ..., n(\\mathbf{X} \\cap B_k)\\) предствляют собой независимые случайные величины;независимость: для неперекрывающихся выборочных областей \\(B_1, B_2, ..., B_k\\), количества \\(n(\\mathbf{X} \\cap B_1), ..., n(\\mathbf{X} \\cap B_k)\\) предствляют собой независимые случайные величины;распределение Пуассона: число \\(n(\\mathbf{B} \\cap B)\\) случайных точек, попадающих в выборочную область \\(B\\) распределено по закону Пуассона.распределение Пуассона: число \\(n(\\mathbf{B} \\cap B)\\) случайных точек, попадающих в выборочную область \\(B\\) распределено по закону Пуассона.Свойства пуассоновского процесса:условность (conditional): в любой области \\(B\\) точки процесса независимо и равномерно распределены;условность (conditional): в любой области \\(B\\) точки процесса независимо и равномерно распределены;прореживаемость (thinning): при случайном прореживании (отборе точек) пуассоновского точечного паттерна с интенсивностью \\(\\lambda\\) результирующий паттерн будет соответствовать Пуассоновскому процессу с интенсивностью \\(p\\lambda\\), где \\(p\\) — вероятность сохранения точки (процент отбора);\n\nBaddeley et. al., 2016\nпрореживаемость (thinning): при случайном прореживании (отборе точек) пуассоновского точечного паттерна с интенсивностью \\(\\lambda\\) результирующий паттерн будет соответствовать Пуассоновскому процессу с интенсивностью \\(p\\lambda\\), где \\(p\\) — вероятность сохранения точки (процент отбора);Baddeley et. al., 2016суперпозиция (superposition): сумма двух независимых гомогенных случайных точечных процессов \\(Z = X \\cup Y\\) с интенсивностями \\(\\lambda_X\\) и \\(\\lambda_Y\\) является гомогенным Пуассоновским процессом с интенсивностью \\(\\lambda_Z = \\lambda_X + \\lambda_Y\\)\n\nBaddeley et. al., 2016\nсуперпозиция (superposition): сумма двух независимых гомогенных случайных точечных процессов \\(Z = X \\cup Y\\) с интенсивностями \\(\\lambda_X\\) и \\(\\lambda_Y\\) является гомогенным Пуассоновским процессом с интенсивностью \\(\\lambda_Z = \\lambda_X + \\lambda_Y\\)Baddeley et. al., 2016","code":""},{"path":"points.html","id":"симуляция-пуассоновского-процесса","chapter":"Глава 4 Точечные паттерны","heading":"4.3.3.1 Симуляция Пуассоновского процесса","text":"Пусть дана область \\(B = [x_{min}, x_{max}] \\times [y_{min}, y_{max}]\\) и интенсивность точечного процесса \\(\\lambda\\).Сгенерировать случайное число \\(N\\), имеющее распределение Пуассона с параметром \\(\\mu = \\lambda |B|\\).Сгенерировать случайное число \\(N\\), имеющее распределение Пуассона с параметром \\(\\mu = \\lambda |B|\\).Сгенерировать \\(N\\) координат \\(X\\), имеющих равномерное распределение на промежутке \\([x_{min}, x_{max}]\\).Сгенерировать \\(N\\) координат \\(X\\), имеющих равномерное распределение на промежутке \\([x_{min}, x_{max}]\\).Сгенерировать \\(N\\) координат \\(Y\\), имеющих равномерное распределение на промежутке \\([y_{min}, y_{max}]\\).Сгенерировать \\(N\\) координат \\(Y\\), имеющих равномерное распределение на промежутке \\([y_{min}, y_{max}]\\).Вероятность получить 0 точек также существует и равна \\(\\mathbb{P}\\{N = 0\\} = e^{-\\mu} \\frac{\\mu^0}{0!} = e^{-\\lambda |B|}\\)","code":""},{"path":"points.html","id":"неоднородный-пуассоновский-процесс","chapter":"Глава 4 Точечные паттерны","heading":"4.3.4 Неоднородный пуассоновский процесс","text":"Определяется следующими параметрами:функция интенсивности: количество \\(n(\\mathbf{X} \\cap B)\\) случайных точек, попадающих в выборочную область \\(B\\) характеризуется мат. ожиданием \\(\\mathbb{E}n(\\mathbf{X} \\cap B) = \\int_B \\lambda(u) du = \\mu\\), где \\(\\lambda(u)\\) есть пространственная функция интенсивности;функция интенсивности: количество \\(n(\\mathbf{X} \\cap B)\\) случайных точек, попадающих в выборочную область \\(B\\) характеризуется мат. ожиданием \\(\\mathbb{E}n(\\mathbf{X} \\cap B) = \\int_B \\lambda(u) du = \\mu\\), где \\(\\lambda(u)\\) есть пространственная функция интенсивности;независимость: для неперекрывающихся выборочных областей \\(B_1, B_2, ..., B_k\\), количества \\(n(\\mathbf{X} \\cap B_1), ..., n(\\mathbf{X} \\cap B_k)\\) предствляют собой независимые случайные величины;независимость: для неперекрывающихся выборочных областей \\(B_1, B_2, ..., B_k\\), количества \\(n(\\mathbf{X} \\cap B_1), ..., n(\\mathbf{X} \\cap B_k)\\) предствляют собой независимые случайные величины;распределение Пуассона: число \\(n(\\mathbf{B} \\cap B)\\) случайных точек, попадающих в выборочную область \\(B\\) распределено по закону Пуассона.распределение Пуассона: число \\(n(\\mathbf{B} \\cap B)\\) случайных точек, попадающих в выборочную область \\(B\\) распределено по закону Пуассона.Данные параметры отличаются следующими свойствами:функция плотности \\(\\lambda(u)\\) определяет общее количество точек и их пространственное распределение;функция плотности \\(\\lambda(u)\\) определяет общее количество точек и их пространственное распределение;никаких ограничений на функцию \\(\\lambda(u)\\) не накладывается, вследствие этого модель неоднородного Пуассоновского процесса является достаточно общей;никаких ограничений на функцию \\(\\lambda(u)\\) не накладывается, вследствие этого модель неоднородного Пуассоновского процесса является достаточно общей;свойства условности, прореживаемости и суперпозиции справедливы также и для неоднородного пуассоновского процесса;свойства условности, прореживаемости и суперпозиции справедливы также и для неоднородного пуассоновского процесса;","code":""},{"path":"points.html","id":"симуляция-неоднородного-пуассоновского-процесса","chapter":"Глава 4 Точечные паттерны","heading":"4.3.4.1 Симуляция неоднородного пуассоновского процесса","text":"Метод Льюиса-Шедлера (Lewis-Shedler thinning):Генерируется однородный Пуассоновской процесс с интенсивностью \\(M = \\max_L\\big[\\lambda(u)\\big]\\).Генерируется однородный Пуассоновской процесс с интенсивностью \\(M = \\max_L\\big[\\lambda(u)\\big]\\).Осуществляется случайное прореживание (исключение) точек с вероятностью сохранения точки \\(p(u) = \\lambda(u) / M\\), пропорциональной функции интенсивности.Осуществляется случайное прореживание (исключение) точек с вероятностью сохранения точки \\(p(u) = \\lambda(u) / M\\), пропорциональной функции интенсивности.Чтобы понять, будет ли точка исключена, генерируется случайное число 0 или 1, имющее распределени Бернулли с вероятностью положительного исхода \\(p = p(u)\\). Это можно сделать с помощью функции rbinom(1, 1, p)\\[\\lambda(x,y) = x + y^2\\]","code":""},{"path":"points.html","id":"процесс-кокса-cox-process","chapter":"Глава 4 Точечные паттерны","heading":"4.3.5 Процесс Кокса (Cox process)","text":"Процесс Кокса определяется как Пуассоновский процесс со случайной функцией интенсивности \\(\\Lambda(u)\\), которая называется порождающей интенсивностьюBaddeley et. al., 2016Слева — реализация случайной функции \\(\\Lambda(u)\\). По центру — реализация Пуассоновского точечного процесса с интенсивностью \\(\\Lambda(u)\\).Cмешанный Пуассоновский процесс: однородный Пуассоновский процесс, порождаемый постоянной случайной величиной \\(\\Lambda\\). Интенсивность процесса равна \\(\\lambda = \\mathbb E\\Lambda\\).Cмешанный Пуассоновский процесс: однородный Пуассоновский процесс, порождаемый постоянной случайной величиной \\(\\Lambda\\). Интенсивность процесса равна \\(\\lambda = \\mathbb E\\Lambda\\).Логнормальный процесс Кокса: процесс Кокса с порождающей интенсивностью, равной \\(\\Lambda(u) = \\exp\\big[G(u)\\big]\\), где \\(G(u)\\) — Гауссовское случайное поле.Логнормальный процесс Кокса: процесс Кокса с порождающей интенсивностью, равной \\(\\Lambda(u) = \\exp\\big[G(u)\\big]\\), где \\(G(u)\\) — Гауссовское случайное поле.Для гауссовского случайного поля в каждой точке \\(u_i\\) случайная величина \\(G(u_i)\\) имеет нормальное распределение, для пары точек \\(u\\) и \\(v\\) пара величин \\(\\big(G(u), G(v)\\big)\\) имеет двумерное нормальное распределение. Аналогично и для произвольного числа точек.Независимые реализации логнормального процесса Кокса:Baddeley et. al., 2016","code":""},{"path":"points.html","id":"кластерные-процессы","chapter":"Глава 4 Точечные паттерны","heading":"4.3.6 Кластерные процессы","text":"Генерируется «родительский» точечный процесс \\(\\mathbf{Y}\\).Генерируется «родительский» точечный процесс \\(\\mathbf{Y}\\).Каждая точка родительского процесса \\(y_i\\) порождает случайный точечный паттерн «потомков» \\(x_{ij}\\)Каждая точка родительского процесса \\(y_i\\) порождает случайный точечный паттерн «потомков» \\(x_{ij}\\)Baddeley et. al., 2016— Наблюдаются только точки потомков (каждая родительская точка замещается ее потомками).— Множество точек \\(x_{ij}\\) формирует реализацию кластерного точечного процесса \\(\\mathbf{X}\\).Baddeley et. al., 2016Возможные свойства кластерных процессов:(CLP1) пуассоновские родители: родительские точки являются реализацией Пуассоновского процесса.(CLP1) пуассоновские родители: родительские точки являются реализацией Пуассоновского процесса.(CLP2) независимые кластеры: кластеры независимы друг от друга.(CLP2) независимые кластеры: кластеры независимы друг от друга.(CLP3) идентично распределенные кластеры: если совместить разные кластеры, то они будут иметь одно распределение.(CLP3) идентично распределенные кластеры: если совместить разные кластеры, то они будут иметь одно распределение.(CLP4) независимые потомки: потомки внутри каждого кластера независимы и одинаково распределены.(CLP4) независимые потомки: потомки внутри каждого кластера независимы и одинаково распределены.Процессы, отвечающие требованиям (CLP1)—(CLP4) носят название процессов Неймана-Скотта (Neyman-Scott).(CLP5) пуассоновское количество потомков: для каждой родительской точки количество потомков есть пуассоновская случайная величина.(CLP5) пуассоновское количество потомков: для каждой родительской точки количество потомков есть пуассоновская случайная величина.(CLP6) изотропные кластеры: плотность распределения потомков зависит только от расстояния до родительской точки.(CLP6) изотропные кластеры: плотность распределения потомков зависит только от расстояния до родительской точки.Распространенные частные случаи:Процесс Матерна \\((\\kappa, \\mu, r)\\): процесс \\(Y\\) имеет интенсивность \\(κ\\), каждый родитель имеет \\(Π(μ)\\) потомков, случайно распределенных в радиусе \\(r\\)\nРеализации процесса Матерна в квадрате \\([0, 1] \\times [0, 1]\\) с интенсивностью родителей \\(\\kappa = 8\\), средним количеством потомков \\(\\mu = 5\\) и радиусом кластера \\(R = 0.1\\):\n\nBaddeley et. al., 2016\nПроцесс Матерна \\((\\kappa, \\mu, r)\\): процесс \\(Y\\) имеет интенсивность \\(κ\\), каждый родитель имеет \\(Π(μ)\\) потомков, случайно распределенных в радиусе \\(r\\)Реализации процесса Матерна в квадрате \\([0, 1] \\times [0, 1]\\) с интенсивностью родителей \\(\\kappa = 8\\), средним количеством потомков \\(\\mu = 5\\) и радиусом кластера \\(R = 0.1\\):Baddeley et. al., 2016Процесс Томаса \\((\\kappa, \\mu, \\sigma)\\): процесс \\(Y\\) имеет интенсивность κ, каждый родитель имеет \\(Π(μ)\\) потомков, смещенных на расстояние, подчиняющееся распределению \\(N(0,\\sigma^2)\\)Процесс Томаса \\((\\kappa, \\mu, \\sigma)\\): процесс \\(Y\\) имеет интенсивность κ, каждый родитель имеет \\(Π(μ)\\) потомков, смещенных на расстояние, подчиняющееся распределению \\(N(0,\\sigma^2)\\)","code":""},{"path":"points.html","id":"регулярные-процессы","chapter":"Глава 4 Точечные паттерны","heading":"4.3.7 Регулярные процессы","text":"Точки не могут располагаться на расстоянии ближе чем \\(r\\) — расстояния ингибиции (отталкивания).Последовательные модели: точки генерируются последовательно согласно Пуассоновскому распредедению (координаты равномерно распределены). Каждая последующая точка сохраняется, только если она находится на расстоянии не ближе, чем \\(r\\).Последовательные модели: точки генерируются последовательно согласно Пуассоновскому распредедению (координаты равномерно распределены). Каждая последующая точка сохраняется, только если она находится на расстоянии не ближе, чем \\(r\\).Зависимое прореживание: генерируется Пуассоновский процесс. После этого удаляются точки, расположенные на расстоянии меньшем \\(r\\). Пары близко расположенных точек аннигилируют (процесс Матерна ). Либо точки маркируются случайным «временем прибытия» и удаляется точка, имеющая более позднее время прибытия (процесс Матерна II).Зависимое прореживание: генерируется Пуассоновский процесс. После этого удаляются точки, расположенные на расстоянии меньшем \\(r\\). Пары близко расположенных точек аннигилируют (процесс Матерна ). Либо точки маркируются случайным «временем прибытия» и удаляется точка, имеющая более позднее время прибытия (процесс Матерна II).Последовательная модель:Baddeley et. al., 2016Процесс Матерна :Baddeley et. al., 2016Процесс Матерна II:","code":""},{"path":"points.html","id":"диагностика","chapter":"Глава 4 Точечные паттерны","heading":"4.4 Диагностика","text":"В предыдущем разделе мы рассмотрели основные типы точечных процессов на плоскости, а также методы их симуляции в виде случайных распределений. В настоящем параграфе показаны различные методы, с помощью которых можно установить, к какому типу процессов относятся наблюдаемые в реальности точки.","code":""},{"path":"points.html","id":"анализ-интенсивности","chapter":"Глава 4 Точечные паттерны","heading":"4.4.1 Анализ интенсивности","text":"","code":""},{"path":"points.html","id":"анализ-зависимости","chapter":"Глава 4 Точечные паттерны","heading":"4.4.2 Анализ зависимости","text":"","code":""},{"path":"points.html","id":"моделирование","chapter":"Глава 4 Точечные паттерны","heading":"4.5 Моделирование","text":"Настоящий раздел посвящен подбору параметров модели точечного процесса для фактических данных о размещении объектов.","code":""},{"path":"points.html","id":"questions_tasks_patterns","chapter":"Глава 4 Точечные паттерны","heading":"4.6 Контрольные вопросы и упражнения","text":"","code":""},{"path":"points.html","id":"questions_patterns","chapter":"Глава 4 Точечные паттерны","heading":"4.6.1 Вопросы","text":"Дайте определение точечного паттерна и точечного процесса.Перечислите три основных типа точечных процессов.Какими особенностями обладают конечный и локально конечный точечный процесс?Каким уравнением опрпеделяется плотность распределения для равномерно случайного точечного процесса в области \\(W\\)? Чему будет равна вероятность попадания точки в подобласть \\(B\\) этой области?Сформулируйте определение биномиального точечного процесса. Какие условия должны выполняться для процесса данного типа?Какими свойствами характеризуется однородный пуассоновский точечный процесс? Как эти свойства можно выразить математически?Перечислите основные параметры пуассоновского точечного процесса.Изложите алгоритм симуляции однородного пуассоновского точечного процесса. Назовите программные средства R, которые можно использовать для этого.Чем неоднородный пуассоновский процесс отличается от однородного? Сформулируйте отличия словесно и математически.Изложите алгоритм симуляции неоднородного пуассоновского точечного процесса по методу Льюиса-Шедлера. Назовите программные средства R, которые можно использовать для этого.Дайте определение процессу Кокса. Приведите два примера частных случаев процесса Кокса.Перечислите концептуальные принципы, лежащие в основе теории кластерных точечных процессов.Назовите шесть основных свойств, с помощью которых определяется характер кластерного точечного процесса.Какими свойствами обладают процессы Неймана-Скотта?Дайте определение кластерных процессов Матерна и Томаса.Какие программные средства имеются в среде R для анализа и симуляции кластерных точечных процессов?Сформулируйте определение регулярного точечного процесса. Каков его основной параметр? Приведите примеры регулярных процессов в географической среде.Назовите два типа моделей, используемых для представления регулярных точечных процессов. Изложите кратко используемые в них алгоритмы симуляции.Перечислите основные статистические функции, используемые для анализа точечных паттернов.Что из себя представляет индекс Моришита? Приведите формулу для его вычисления и раскройте суть каждой компоненты этой формулы.В чем заключается преимущество K-функции Рипли в сравенении с другими методами оценки характера точечных паттернов?","code":""},{"path":"points.html","id":"tasks_patterns","chapter":"Глава 4 Точечные паттерны","heading":"4.6.2 Упражнения","text":"","code":""}]
